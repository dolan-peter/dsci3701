<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<meta name="progressive" content="false" />
<meta name="allow-skip" content="false" />

<title>Tutorial</title>


<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<div id="section-clustering" class="section level2">
<h2>Clustering</h2>
<p>There are many approaches to clustering. We will look at the following:</p>
<ul>
<li>Classification trees using `method=“anova” (supervised learning)</li>
<li><span class="math inline">\(k\)</span> nearest neighbor clustering (supervised learning)</li>
<li><span class="math inline">\(k\)</span>-means clustering (unsupervised)</li>
<li>Naive Bayes</li>
</ul>
</div>
<div id="section-classification-trees" class="section level2">
<h2>Classification Trees</h2>
<p>See Linear Regression Tutorial.</p>
</div>
<div id="section-k-nearest-neighbor" class="section level2">
<h2><span class="math inline">\(k\)</span> nearest neighbor</h2>
<p>ADD LAZY DISCUSSION (chapter 3 machine learning with R)</p>
<p>Suppose we have data in table form. One attribute is the clustering attribute the rest act as explanatory variables. What we want to do is use the knowledge that we already have to make an educated guess as to the category to which some other element belongs. This can be done for binary categories as easily as for any other, so let’s use the UCLA data-set that we have previously examined:</p>
<p>` Logistic regression would allow us to predict the odds</p>
<p>Read <a href="https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/">this tutorial</a></p>
<p>You will be asked to do the R portions from the article in the exercise below so you might want to do type along while you are reading the article.</p>
<p>The Python has gotten a little bit out of date, so I will update it and include a Python section with the appopriate information below. You should type the Python into a Jupyter notebook. As always you should type the material yourself instead of using copy-and-paste.</p>
<p>I find it helpful (whether in R or in Python) to use <code>wget</code> from the corresponding terminal (and with the ssh-session in the proper working directory). You use <code>wget</code> to download the file:</p>
<pre class="python"><code># load libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from collections import Counter

# define column names
names = [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;, &#39;petal_width&#39;, &#39;class&#39;]

# load data
df = pd.read_csv(&#39;~/dsci3701/Clustering/iris.data&#39;, header=None, names=names)
df.head()
# create design matrix X and target vector y
# note the use of np arrays AND pandas dataframe:</code></pre>
<pre><code>##    sepal_length  sepal_width  petal_length  petal_width        class
## 0           5.1          3.5           1.4          0.2  Iris-setosa
## 1           4.9          3.0           1.4          0.2  Iris-setosa
## 2           4.7          3.2           1.3          0.2  Iris-setosa
## 3           4.6          3.1           1.5          0.2  Iris-setosa
## 4           5.0          3.6           1.4          0.2  Iris-setosa</code></pre>
<pre class="python"><code>X = np.array(df.ix[:, 0:4])     # end index is exclusive
y = np.array(df[&#39;class&#39;])     # another way of indexing a pandas df

# split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
# instantiate learning model (k = 3)
knn = KNeighborsClassifier(n_neighbors=3)

# fit the model
knn.fit(X_train, y_train)

# predict the response</code></pre>
<pre><code>## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##            metric_params=None, n_jobs=None, n_neighbors=3, p=2,
##            weights=&#39;uniform&#39;)</code></pre>
<pre class="python"><code>pred = knn.predict(X_test)

# evaluate accuracy and share results with knitted document
print accuracy_score(y_test, pred)</code></pre>
<pre><code>## 0.98</code></pre>
<p>Now we will do the second part of the article, starting with the <code>train()</code> function:</p>
<pre class="python"><code>def train(X_train, y_train):
    # do nothing 
    return</code></pre>
<p>The predict function will use <code>X_train</code> and <code>y_train</code> as its data-resource to make a prediction for the category of <code>x_test</code>. The <code>k</code> argument indicates the number of nearest neighbors to consult:</p>
<pre class="python"><code>def predict(X_train, y_train, x_test, k):
    # create list for distances and targets
    distances = []
    targets = []
  #
    for i in range(len(X_train)):
        # first we compute the euclidean distance
        distance = np.sqrt(np.sum(np.square(x_test - X_train[i, :])))
        # add it to list of distances
        distances.append([distance, i])
  #
    # sort the list
    distances = sorted(distances)
  #
    # make a list of the k neighbors&#39; targets
    for i in range(k):
        index = distances[i][1]
        targets.append(y_train[index])
  #
    # return most common target
    return Counter(targets).most_common(1)[0][0]</code></pre>
<p>Here’s where the actual sutff happens</p>
<pre class="python"><code>def kNearestNeighbor(X_train, y_train, X_test, predictions, k):
    # train on the input data (this is really a dummy function)
    train(X_train, y_train)
  #
    # loop over all observations
    for i in range(len(X_test)):
        predictions.append(predict(X_train, y_train, X_test[i, :], k))</code></pre>
<p>Now let’s do it and see what happens</p>
<pre class="python"><code>predictions = []

kNearestNeighbor(X_train, y_train, X_test, predictions, 7)

# transform the list into an array
predictions = np.asarray(predictions)

# evaluating accuracy
accuracy = accuracy_score(y_test, predictions)

print(&#39;\nThe accuracy of our classifier is %d%%&#39; % (accuracy*100))</code></pre>
<pre><code>## 
## The accuracy of our classifier is 98%</code></pre>
<p><strong>Exercise:</strong></p>
<ol style="list-style-type: decimal">
<li>Replicate the python (no copy-pasting) in a Jupyter notebook. (difficulty: easy)</li>
<li>Replicate the R-graphing code from article (difficutly: easy)</li>
<li>Rewrite the functions <code>train()</code>, <code>predict()</code>, <code>kNearestNeighbor()</code> in R (difficulty level: medium)</li>
<li>Create an <code>accuracy_score</code> function (difficulty: medium-hard)</li>
</ol>
<div id="section-details" class="section level3">
<h3>Details</h3>
<p>I want to pull out a few details from the tutorial.</p>
<ol style="list-style-type: decimal">
<li>The larger the value of <span class="math inline">\(k\)</span> the the more neighbors are consulted in the voting process and the more the process resembles “choose the most common”. The lower the value of <span class="math inline">\(k\)</span> the more variance and sensitivity to outliers.</li>
<li>The <strong>scale</strong> of a variable has a strong influence on the distance function. Without a compelling reason (like a shared scale) to do otherwise you will probably want to standardize your scales. Common choice include <span class="math inline">\(min-max\)</span>-scaling <span class="math inline">\(\left(x_i&#39;=\frac{x_i-\textrm{min}}{\textrm{max}-\textrm{min}}\right)\)</span> or <span class="math inline">\(z\)</span>-scoring: <span class="math inline">\(z_i = \frac{x_i - \overline{x}}{s_x}\)</span> where <span class="math inline">\(s_x\)</span> is the standard deviation of the variable.</li>
<li>Pay very careful attention to categorical variables– if they are imported as factors (The default) then, under the hood, there is a number associated to each level– the normal Euclidean distance ideas may be meaningless in such situations.</li>
</ol>
<p>Let’s apply this techniqe to the UCLA entrance data. Recall</p>
<table>
<thead>
<tr class="header">
<th>Variable</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>admit</td>
<td>1-hot coding</td>
</tr>
<tr class="even">
<td>gre</td>
<td>numeric</td>
</tr>
<tr class="odd">
<td>gpa</td>
<td>numeric</td>
</tr>
<tr class="even">
<td>rank</td>
<td>defaults in <code>read.csv()</code> as numeric.</td>
</tr>
</tbody>
</table>
<p>The <code>admit</code> variable is, of course, what I am most interested in being able to predict. As we saw in the <em>Regression Tutorial</em> we will want to normalize our variables. We will do that down below</p>
<p>Let’s get the data into a dataframe:</p>
<pre><code>## [1] &quot;admit&quot; &quot;gre&quot;   &quot;gpa&quot;   &quot;rank&quot;</code></pre>
<p>Now let’s normalize:</p>
<p>Our data set isn’t particularly large. Let’s use 2/3 for the training and 1/3 for the test set. There are lots of ways to do this. I’m going to use the <code>sample()</code> function to choose 1/3 of the rows numbers. I’ll create a <code>data</code> list and a <code>true.values</code> list. Both will hold two entries: One for “train” and one for “test”.</p>
<p>Also, to avoid confusion later I am going to change the <code>admit</code> variable so that it holds the values “reject” and “admit”.</p>
<p>We now have a <em>list</em> called <code>data</code> with two entries (both of which are data.frames)</p>
<ul>
<li><code>data$train</code></li>
<li><code>data$test</code></li>
</ul>
<p>We have a similar one called <code>true.values</code>.</p>
<p>We will use the <code>knn()</code> function from the package <code>class</code>:</p>
<pre><code>## [1] admit  reject reject reject admit  admit 
## Levels: admit reject</code></pre>
<p>There are just as many results as there are entries in the <code>test</code> data set. Let’s make the confusion matrix and see what our accuracy looks like:</p>
<pre><code>## [1] 0.6466165</code></pre>
<p>Let’s calculate the accuracy for all possible values of <span class="math inline">\(k\)</span></p>
<p><img src="Clustering_files/figure-html/unnamed-chunk-13-1.png" width="624" /> As you can see, after a certain point the process settles down to a constant accuracy– that’s, not surprisingly the result of just voting “reject” on all of the testing data is what dominates when <span class="math inline">\(k\)</span> is large enough.</p>
<p>Why is that? Well, for one thing, by standardizing the results we, essentially, gave each variable the same weight in the decision making process… but we know that’s not necessarily a good idea.</p>
<p>This same weakness can also manifest, surprisingly, in situations with high dimensionality (translate that last phrase as “many variables”). As more variables are introduced, a variable’s ability to make enough of a difference to distinguish between cases can be swamped out by random noise in the remaining variables.</p>
<p>One of the most common datasets (and one that is included in R) is the <code>Iris</code> data set used by one of the founders of modern biological statistics– Fisher</p>
<p>In an R console type <code>?iris</code> and learn the details.</p>
<p>You’ll want to do some graphic explorations. I’ll start with some information regarding the sepal:</p>
<p><img src="Clustering_files/figure-html/unnamed-chunk-14-1.png" width="624" /><img src="Clustering_files/figure-html/unnamed-chunk-14-2.png" width="624" /><img src="Clustering_files/figure-html/unnamed-chunk-14-3.png" width="624" /></p>
<p><strong>Exercise:</strong> Repeat the last example using Iris. You are categorizing off of <code>species</code>. Make an accuracy plot. Notice that quite rapidly an increasing <span class="math inline">\(k\)</span> dramatically reduces accuracy:</p>
<p><strong>BONUS:</strong> Do it for both forms of normalization (you might find the R function <code>scale()</code> helpful)</p>
<p><img src="Clustering_files/figure-html/iris-exercise%20exercise-1.png" width="624" /></p>
</div>
</div>
<div id="section-k-means-clustering" class="section level2">
<h2><span class="math inline">\(k\)</span>-means clustering</h2>
<p>In <span class="math inline">\(k\)</span>-nearest neighbor we already know the category to which the row should belong. This is an example of <strong>supervised</strong> learning. IF we did NOT know, but suspected there was some deeper structure, we might use a “clustering” algorithm.</p>
<p>The hope is that we will be able to naturally find groups. I am going to heavily borrow from <a href="https://uc-r.github.io/kmeans_clustering" class="uri">https://uc-r.github.io/kmeans_clustering</a>.</p>
<p>In this IRIS example, we might try various values of <span class="math inline">\(k\)</span> and assess the clustering for various values of <span class="math inline">\(k\)</span>. We will need the packages <code>cluster</code> and <code>factoextra</code>. We are going to need to normalize. In the last section we, used min-max normalization in the example, but I asked you (as extra) to use both forms of standardization. For small values of <span class="math inline">\(k\)</span> they are essentially the same, particularly for small <span class="math inline">\(k\)</span>, atlhough there were some intriquing patterns in the accuracy that could be fun to explore. For this one, doing the z-score normalization makes the most sense to me– the value are numeric</p>
<p>However the</p>
<pre><code>## Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ</code></pre>
<p>The <code>factoextra</code> package has the <code>fviz_dist</code> function:</p>
<p><img src="Clustering_files/figure-html/unnamed-chunk-16-1.png" width="624" /></p>
<p>Another common visualization function (really the same basic idea) is <code>heatmap()</code></p>
<p><img src="Clustering_files/figure-html/unnamed-chunk-17-1.png" width="624" /></p>
<p><code>heatmap()</code> has the advantage of produce <strong>dendrograms</strong> (tree diagrams) which are an attempt to do some hieraricical clustering.</p>
<p>The <span class="math inline">\(k\)</span>-means algorithm, and I’m sure you’re getting tired of this… tries to choose centers of groups in order to minimize a sum-of-squares error function. Since we have an advantage and already know that there are 3 clusters… let’s do this and see how good it does. First we’ll do the clustering (<code>nstart=25</code> is recommended by our tutorial)</p>
<p>The <code>cluster</code> object has many components that we can access:</p>
<pre><code>## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;    
## [5] &quot;tot.withinss&quot; &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;        
## [9] &quot;ifault&quot;</code></pre>
<p>The tutorial has more details. We’ll look at <code>cluster$cluster</code> and compare them to the true species:</p>
<pre><code>##    
##     setosa versicolor virginica
##   1      0         11        36
##   2     50          0         0
##   3      0         39        14</code></pre>
<p>We have to look carefully to decide which cluster should be associated with each species, but it’s pretty clear:</p>
<table>
<thead>
<tr class="header">
<th>Cluster</th>
<th>species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>setosa</td>
</tr>
<tr class="even">
<td>2</td>
<td>versicolor</td>
</tr>
<tr class="odd">
<td>3</td>
<td>virginica</td>
</tr>
</tbody>
</table>
<p>Luckily these values fell on the diagonal, so we can calculate the accuracy:</p>
<pre><code>## [1] 0.09333333</code></pre>
<p>Not to shabby. Not as good as the $k-nearest neighbor approach, but given that this approach does not know the species it’s pretty good. Particularly <code>setosa</code> which clearly was easily grouped.</p>
<p>Let’s use the <code>fviz_cluster()</code> function with the original data values:</p>
<p><img src="Clustering_files/figure-html/unnamed-chunk-22-1.png" width="624" /></p>
<p>That separation between 3 (<code>setosa</code>) and the others is very strong… but if we limit our attention to just <code>versiolor</code> and <code>virginica</code>, Our accuracy is closer to 75%</p>
<p>So what are we seeing?</p>
<p>ADD MORE HERE! ADD 3-FOLD cross validation here Using the elbow method: <img src="Clustering_files/figure-html/unnamed-chunk-23-1.png" width="624" /></p>
<p>Using the silhouette method:</p>
<p><img src="Clustering_files/figure-html/unnamed-chunk-24-1.png" width="624" /></p>
<p>Interesting… The data clearly thinks that 2 groups is a better idea</p>
<p><img src="Clustering_files/figure-html/unnamed-chunk-25-1.png" width="624" /></p>
<p>This one suggests 2 clusters as well… the moral of the story is… sometimes you need outside knowledge to get it right!</p>
</div>
<div id="section-hierarichical-clustering-methods" class="section level2">
<h2>Hierarichical clustering methods</h2>
</div>
<div id="section-naive-bayes" class="section level2">
<h2>Naive Bayes</h2>
<p>We use the usual machine learning vocabulary to describe this situation– the data is tabular. The emphasis is on rows (which we might call subjects, units, or cases in more traditional statistical vocabulary). In database parlance we would call a row a record.</p>
<p>Instead of discussing <em>variables</em>, we instead refer to <strong>features</strong> or <strong>attributes</strong> (in database language these would be called <strong>fields</strong>).</p>
<p>Bayes Law (see the Mathemtical Background for a more complete refresher). We will use the convention that <span class="math inline">\(\Omega\)</span> is the sample space and <span class="math inline">\(H,E \subset \Omega\)</span> are events. (Here called the Hypothesis and the Evidence) We will also use a superscript <span class="math inline">\(c\)</span>, such as <span class="math inline">\(H^c\)</span> to denote the complement. (I actually prefer the overline– but it’s too easy to confuse with the sample mean). Bayes rule says:</p>
<p><span class="math display">\[
\begin{aligned}
\textrm{P}(H|E) &amp;= \frac{P(E|H)P(H)}{P(E)}=\frac{P(E|H)}{P(E)}P(H)\\
\end{aligned}
%=\frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|A^c)P(A^c)}
\]</span> In Bayesian statistics, probability is measured in terms of <strong>certainty</strong>. I’m using the notation of Wikipedia’s entry on Bayesian inference because I think the <span class="math inline">\(E\)</span> and <span class="math inline">\(H\)</span> help make several things easier to understand.</p>
<p>Think of <span class="math inline">\(H\)</span> as a hypothesis and <span class="math inline">\(E\)</span> as evidence. <span class="math inline">\(P(H)\)</span> represents our <strong>Prior knowledge</strong> about the hypothesis <span class="math inline">\(H\)</span>, which is, in an information sense, equivalent to our knowledge about <span class="math inline">\(H^c\)</span> (having a firm believe about when something will happen is equivalent to having a firm belief about when it will not happen). The ratio <span class="math inline">\(\frac{P(E|H)}{P(E)}\)</span> is the support provided by <span class="math inline">\(E\)</span> for <span class="math inline">\(H\)</span>. The numerator, <span class="math inline">\(P(E|H)\)</span> is also known as the <strong>likelihood</strong>. Finally <span class="math inline">\(P(H|E)\)</span> is the new belief about <span class="math inline">\(H\)</span> after taking <span class="math inline">\(E\)</span> into consideration (this is the posterior distribution).<br />
Say the formula to yourself in words:</p>
<pre><code>Our belief that the hypothesis is true, given the evidence, is our earlier belief in hypothesis adjusted by the relationship between the liklihood of the evidence (under the hypothesis) and our belief about the evidence occuring in general.</code></pre>
<p>Notice that if <span class="math inline">\(E\)</span> and <span class="math inline">\(H\)</span> are <strong>independent</strong> (in the probability sense) then <span class="math inline">\(P(E|H) = P(E)\)</span> and the ratio <span class="math inline">\(\frac{P(E|H)}{P(E)}=\frac{P(E)}{P(E)}=1\)</span> and so our posterior probability, <span class="math inline">\(P(H|E)\)</span> is the same as our prior probability <span class="math inline">\(P(H)\)</span>. In other words, if our belief in the evidence is not influenced by the hypothesis then whether it occurs or does not occur will not influence our assessment of the hypothesis.</p>
<p>On the other hand… if the evidence is more likely under the hypotheis then taken in general (say twice as likely) then we’ll double our belief in the hypothesis.</p>
<p>Now there’s one more piece to the puzzle. We usually don’t enter into thinking about <span class="math inline">\(P(E)\)</span> without considering a hypothesis. So we use the <strong>law of total probability</strong> to rewrite <span class="math inline">\(P(E)\)</span>:</p>
<p><span class="math display">\[
P(E) = P(E|H)P(H) + P(E|H^c)P(H^c)
\]</span></p>
<p>Now we are saying that our belief in the evidence occuring arises from four considerations:</p>
<ol style="list-style-type: decimal">
<li>Our belief in <span class="math inline">\(H\)</span></li>
<li>Our belief in <span class="math inline">\(H^c\)</span></li>
<li>Our belief that the evidence will occur <strong>under</strong> <span class="math inline">\(H\)</span></li>
<li>Our belief taht the evidence will occur <strong>under</strong> <span class="math inline">\(H^c\)</span></li>
</ol>
<p>So….in plain words</p>
<p>We change our old belief about the hypothesis <span class="math inline">\(H\)</span> (<span class="math inline">\(P(H)\)</span>) in the face of evidence <span class="math inline">\(E\)</span> (<span class="math inline">\(P(H|E)\)</span>) by comparing our beliefs about the likelihood of the evidence occuring <strong>under the hypothesis</strong> (<span class="math inline">\(P(E|H)\)</span>) and our beliefs about the evidence occuring no matter what happens to be true <span class="math inline">\(P(E)\)</span>, but our belief about <span class="math inline">\(E\)</span> can be broken down into our belief about <span class="math inline">\(E\)</span> under <span class="math inline">\(H\)</span> and under <span class="math inline">\(H^c\)</span> (<span class="math inline">\(P(E) = P(E|H)P(H) + P(E|H^c)*P(H^c)\)</span>). Notice that our beliefs about the liklihood of <span class="math inline">\(E\)</span> occuring under <span class="math inline">\(H\)</span> and under <span class="math inline">\(H^c\)</span> certainly don’t need to add to 1. As an extreme example, if we believed that <span class="math inline">\(E\)</span> was inevitable… no matter what… then <span class="math inline">\(P(E|H) = 1\)</span> and <span class="math inline">\(P(E|H^c)=1\)</span>.</p>
<p>As a more numeric example, Suppose we were considering the possiblity of a fair die, and of one where even numbers were twice as likely as odd numbers.</p>
<p>For some mysterious reason these are the only two options that are under consideration… perhaps this somewhat contrived scenario involves a magician. In any event, we would have the following probability tables for a single roll (rounded to 2 places and <em>slightly adjusted to add to 1</em>):</p>
<p><strong>situation 1</strong></p>
<table>
<thead>
<tr class="header">
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.17</td>
<td>0.17</td>
<td>0.17</td>
<td>0.17</td>
<td>0.16</td>
<td>0.16</td>
</tr>
</tbody>
</table>
<p><strong>situation 2</strong></p>
<table>
<thead>
<tr class="header">
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.11</td>
<td>0.22</td>
<td>0.11</td>
<td>0.22</td>
<td>0.11</td>
<td>0.23</td>
</tr>
</tbody>
</table>
<p>Now let’s consider rolling two dice and seeing that the sum is 10. Under the first scenario this can only arise when the die rolls are <span class="math inline">\(\left\{(4,6), (5,5), (6,4)\right\}\)</span> since the die rolls are independent the probabilitity that the sum is 3 is <span class="math inline">\(0.17*0.16+0.16*0.16+0.16*0.17=0.08\)</span>.</p>
<p>Under the second scenario it is <span class="math inline">\(0.22*0.22+0.11*0.11+0.23*0.23-0.1133\)</span></p>
<p>Now here is where it gets interesting: Without any math– if we were to observe a sum of 10 on two die rolls we would be slightly more inclined to believe the second hypothesis <span class="math inline">\(H^c\)</span> over the first, <span class="math inline">\(H\)</span>.</p>
<p>Will the math support this? The answer is YES. Remember that <span class="math inline">\(E=\textrm{sum is 10}\)</span>. So we have <span class="math inline">\(P(E|H) = 0.0867\)</span> and <span class="math inline">\(P(E|H^c) = 0.1089\)</span>. No matter what probabilities we assign to <span class="math inline">\(H\)</span>, <span class="math inline">\(P(H)\)</span> is a weighted average of <span class="math inline">\(P(E|H)\)</span> and <span class="math inline">\(P(E|H^c)\)</span> where <span class="math inline">\(P(H)\)</span> and <span class="math inline">\(P(H^c)\)</span> play the role of the scalars in the linear combination. (review math background on this)</p>
<p>So if <span class="math inline">\(P(E|H) \ge P(E|H^c)\)</span>then <span class="math inline">\(P(E|H) \ge P(E)\)</span>. And thus the ratio <span class="math inline">\(\frac{P(E|H)}{P(E)}\ge 1\)</span>. Similarly, if <span class="math inline">\(P(E|H) \le P(E|H^c)\)</span> then <span class="math inline">\(P(E|H) \le P(E)\)</span> and the ratio <span class="math inline">\(\frac{P(E|H)}{P(E)}\le 1\)</span></p>
<p>Also notice that the entire RHS of the equation has an equivalent form:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{P(E|H)}{P(E)}P(H)&amp;=\frac{P(E|H)P(H)}{P(E)} \\
&amp;=\frac{P(E \textrm{ and } H)}{P(E)}
\end{aligned}
\]</span></p>
<p>Clearly <span class="math inline">\(P(E \textrm{ and } H) \le P(E)\)</span> and thus</p>
<p><span class="math display">\[
\begin{aligned}
\frac{P(E|H)}{P(E)}P(H)&amp;=\frac{P(E \textrm{ and } H)}{P(E)}\\
&amp;\le \frac{P(E)}{P(E)}\\
&amp;=\le 1
\end{aligned}
\]</span></p>
<p>Similarly, the ratio must be <span class="math inline">\(\ge 0\)</span>. So no matter how big, or how small the ratio <span class="math inline">\(\frac{P(E|H)}{P(E)}\)</span> the new value, <span class="math inline">\(P(H|E)\)</span> will still be between 0 and 1.</p>
<p><strong>Exercise:</strong> Using your functions from the R review (<code>two.roll.sum</code> and <code>weighted.die</code>) define your two tables as</p>
<p>Start with the assumption that both dice are equally likely and <code>set.seed(10)</code>. Use your function <code>weighted.die</code> to roll a fair die twice and calculate the sum.<br />
Using this sum as evidence and <code>two.roll.sum</code> update your belief in <span class="math inline">\(H\)</span>. Repeat until <span class="math inline">\(P(H)&gt; 0.95\)</span>. How many iterations did it take?</p>
<pre><code>## [1] 227</code></pre>
<p><img src="Clustering_files/figure-html/unnamed-chunk-27-1.png" width="624" /></p>
<p>I hunted around for awhile until I found a seed that brought <span class="math inline">\(H\)</span> below 0.5 (most of the one’s I tried didn’t dip below 0.5)</p>
<p>We can calculate the probability of getting a sum that makes our assessment of <span class="math inline">\(H\)</span> drop by figuring out which sums are more likely under <span class="math inline">\(H^c\)</span>:</p>
<pre><code>## [1] 0.2433</code></pre>
<p>We can also look at the number of improvements vs diminishments:</p>
<pre><code>## 
##  -1   1 
##  58 169</code></pre>
<p>and convert to a proportion:</p>
<pre><code>## [1] 0.2555066</code></pre>
<p>So there is almost a 25% chance that our fair die will generate a result that makes us more likely to believe <span class="math inline">\(H^c\)</span> over <span class="math inline">\(H\)</span>. However, over time, even when purely random we expect, eventually, for <span class="math inline">\(H\)</span> to win-out.</p>
<p>But if our <strong>decision policy</strong> is to <code>accept H if H &gt;0.95$</code>accept H^c if H&lt;0.05` then sometimes we would expect, through random chance, to reject <span class="math inline">\(H\)</span> when we should not <strong>type I error</strong>.</p>
<p>Notice that we have not calculate the probability of a type I error– our situation is more like a random walk– the probability, per trial, of an advantageous result (one that promotes the truth about <span class="math inline">\(H\)</span>) is over 75% and the probability of a disadvantageous result (is less thatn 25%)…</p>
<p>The actual probablility of reaching the 0.95 or 0.05 thresholds of decision depend also upon how much the value of <span class="math inline">\(P(H)\)</span> is perturbed for various pieces of evidence. We’re not going to pursue that here… although we <em>did</em> simulate one run up above.</p>
<p>On the other hand we could a consider how likely it is to generate a result that is disadvantageous to <span class="math inline">\(H^c\)</span>, when <span class="math inline">\(H^c\)</span> is, in fact, the truth. This is related to the type II error.</p>
<pre><code>## [1] 0.66</code></pre>
<p>wow! 66%</p>
<p>Let’s see if we even converge to the true answer:</p>
<pre><code>## [1] 146</code></pre>
<p><img src="Clustering_files/figure-html/unnamed-chunk-32-1.png" width="624" /></p>
<pre><code>## [1] 0.6506849</code></pre>
<p>So… we moved “up” (towards accepting <span class="math inline">\(H\)</span>) far more frequently than down…. but when we moved down the change was more extreme. So in the end, we reached the proper decision threshold anyway.</p>
<p>This phenomenon is important in both data science <strong>and</strong> science in general– and it relates to why no one experiment should be taken too seriously.</p>
<p>##H2O</p>
<p>H2o is an open source, machine learning (and AI) platform. Both Python and R have libraries for interfacing with the software. We will use an example from R. H2o is a java program that provides an API that R and Python use to communicate with it.</p>
<p>To use an H2o R function one needs to first initialize the H2o server. This is done using <code>h2o.init()</code> R will attempt to start an h2o server and connect on the localhost vai the appropriate port. IN order to understand the conventions used in the R interface you need to understand that H2o does not have access to the R memory space:</p>
<pre><code>Note that no actual data is stored in the R workspace; and no actual work is carried out by R. R only saves the named objects, which uniquely identify the data set, model, etc on the server. When the user makes a request, R queries the server via the REST API, which returns a JSON file with the relevant information that R then displays in the console.</code></pre>
<p>That means that the basic usage has four steps:</p>
<ol style="list-style-type: decimal">
<li>Initialize the H2o engine/server</li>
<li>Transfer files to the server</li>
<li>Perform the operations</li>
<li>Get the data</li>
</ol>
<p>That last one is a bit subtle– the R objects returned by the R H2o functions are indeed, R objects, but they interact with the H2o server to pull data <strong>as necessary</strong>. If the object is saved, that does not mean that the connection is maintained… Often this is the best solution– why make two copies of the data (particularly if the data set is large)? But somtimes the final reslt of along analysis is a reasonably sized set of data.. in that case it’s nice to pull the data into a local R object so that it can be saved in a manner that is independent of the H2o server and it’s connection to RStudio.</p>
<p>Let’s apply this to the <code>iris</code> data we have been using. Since the h2o package has a copy of the file (it’s popular in machine learning circles) we can use <code>system.file()</code> to find the file’s location and upload it:</p>
<pre><code>## 
## ----------------------------------------------------------------------
## 
## Your next step is to start H2O:
##     &gt; h2o.init()
## 
## For H2O package documentation, ask for help:
##     &gt; ??h2o
## 
## After starting H2O, you can use the Web UI at http://localhost:54321
## For more information visit http://docs.h2o.ai
## 
## ----------------------------------------------------------------------</code></pre>
<pre><code>## 
## Attaching package: &#39;h2o&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cor, sd, var</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     &amp;&amp;, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,
##     colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,
##     log10, log1p, log2, round, signif, trunc</code></pre>
<pre><code>##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         2 hours 56 minutes 
##     H2O cluster timezone:       America/Chicago 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.24.0.5 
##     H2O cluster version age:    1 month and 12 days  
##     H2O cluster name:           H2O_started_from_R_dolanp_blu152 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   26.08 GB 
##     H2O cluster total cores:    16 
##     H2O cluster allowed cores:  16 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.3 (2019-03-11)</code></pre>
<p>Now we find the path, upload it, and store the relevant access information in an R object for future interactions with H2o:</p>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre><code>## Warning in write.csv(tmp$train, file = &quot;iristrain.csv&quot;, row.names =
## FALSE, : attempt to set &#39;col.names&#39; ignored</code></pre>
<pre><code>## Warning in write.csv(tmp$test, file = &quot;iristest.csv&quot;, row.names = FALSE, :
## attempt to set &#39;col.names&#39; ignored</code></pre>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre><code>## Model Details:
## ==============
## 
## H2OMultinomialModel: naivebayes
## Model Key:  NaiveBayes_model_R_1564579412771_5 
## Model Summary: 
##   number_of_response_levels min_apriori_probability
## 1                         3                 0.32000
##   max_apriori_probability
## 1                 0.36000
## 
## H2OMultinomialMetrics: naivebayes
## ** Reported on training data. **
## 
## Training Set Metrics: 
## =====================
## 
## Extract training frame with `h2o.getFrame(&quot;iristrain.hex_sid_98eb_2&quot;)`
## MSE: (Extract with `h2o.mse`) 0.03331574
## RMSE: (Extract with `h2o.rmse`) 0.182526
## Logloss: (Extract with `h2o.logloss`) 0.1295557
## Mean Per-Class Error: 0.03935185
## Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,train = TRUE)`)
## =========================================================================
## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
##            setosa versicolor virginica  Error      Rate
## setosa         32          0         0 0.0000 =  0 / 32
## versicolor      0         34         2 0.0556 =  2 / 36
## virginica       0          2        30 0.0625 =  2 / 32
## Totals         32         36        32 0.0400 = 4 / 100
## 
## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,train = TRUE)`
## =======================================================================
## Top-3 Hit Ratios: 
##   k hit_ratio
## 1 1  0.960000
## 2 2  1.000000
## 3 3  1.000000
## 
## 
## H2OMultinomialMetrics: naivebayes
## ** Reported on validation data. **
## 
## Validation Set Metrics: 
## =====================
## 
## Extract validation frame with `h2o.getFrame(&quot;iristest.hex_sid_98eb_3&quot;)`
## MSE: (Extract with `h2o.mse`) 0.03429075
## RMSE: (Extract with `h2o.rmse`) 0.1851776
## Logloss: (Extract with `h2o.logloss`) 0.1035551
## Mean Per-Class Error: 0.03703704
## Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;,valid = TRUE)`)
## =========================================================================
## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
##            setosa versicolor virginica  Error     Rate
## setosa         18          0         0 0.0000 = 0 / 18
## versicolor      0         14         0 0.0000 = 0 / 14
## virginica       0          2        16 0.1111 = 2 / 18
## Totals         18         16        16 0.0400 = 2 / 50
## 
## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;,valid = TRUE)`
## =======================================================================
## Top-3 Hit Ratios: 
##   k hit_ratio
## 1 1  0.960000
## 2 2  1.000000
## 3 3  1.000000
## 
## 
## 
## 
## NULL</code></pre>
<p>Naive Bayes Look at the confusion matrix. Not too shabby is it? (18+14+16)/(18+14+16+2)=96 percent accuracy</p>
<p>You s</p>
<p>Naive Bayes is often used in spam detection. The idea is that a collection of words is gathered from a large setof emails. A large table is produced where the value is 0 if the word is missing and a 1 if the word is present.</p>
<p>##Add shorcomings</p>

<script type="application/shiny-prerendered" data-context="server-start">
library(learnr)
library(reticulate)
knitr::opts_chunk$set(echo = FALSE)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::register_http_handlers(session, metadata = NULL)
</script>
 <!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/cerulean.min.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css","rstudio-theme.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["font-awesome"]},{"type":"character","attributes":{},"value":["5.1.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/fontawesome"]}]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["css/all.css","css/v4-shims.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["4.4.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["localforage"]},{"type":"character","attributes":{},"value":["1.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/localforage"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["localforage.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66]}},"value":[{"type":"character","attributes":{},"value":["assertthat","backports","base","bitops","class","cluster","colorspace","compiler","crayon","datasets","digest","dplyr","evaluate","factoextra","ggplot2","ggpubr","ggrepel","ggsignif","glue","graphics","grDevices","grid","gtable","h2o","htmltools","htmlwidgets","httpuv","jsonlite","knitr","labeling","later","lattice","lazyeval","learnr","magrittr","markdown","Matrix","methods","mime","munsell","pillar","pkgconfig","plyr","promises","purrr","R6","Rcpp","RCurl","reshape2","reticulate","rlang","rmarkdown","rprojroot","scales","shiny","stats","stringi","stringr","tibble","tidyselect","tools","utils","withr","xfun","xtable","yaml"]},{"type":"character","attributes":{},"value":["0.2.1","1.1.4","3.5.3","1.0-6","7.3-15","2.1.0","1.4-1","3.5.3","1.3.4","3.5.3","0.6.19","0.8.1","0.14","1.0.5","3.2.0","0.2.1","0.8.1","0.5.0","1.3.1","3.5.3","3.5.3","3.5.3","0.3.0","3.24.0.5","0.3.6","1.3","1.5.1","1.6","1.23","0.3","0.8.0","0.20-38","0.2.2","0.9.2.1","1.5","1.0","1.2-17","3.5.3","0.7","0.5.0","1.4.1","2.0.2","1.8.4","1.0.1","0.3.2","2.4.0","1.0.2","1.95-4.12","1.4.3","1.12.0-9007","0.4.0","1.13","1.3-2","1.0.0","1.3.2","3.5.3","1.4.3","1.4.0","2.1.3","0.2.5","3.5.3","3.5.3","2.1.2","0.7","1.8-4","2.2.0"]}]}]}
</script>
<!--/html_preserve-->
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">Tutorial</h2>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>

</html>
