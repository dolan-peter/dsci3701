---
title: "Regression Tutorial"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE)
# knitr::knit_hooks$set(output = function(x, options){
#   if(!is.null(options$max_height)){
#     paste('<pre style = "max-height:',
#           options$max_height, 
#           ';float: left; width: 910px; overflow-y: auto;">',
#           x,
#           "</pre>",
#           sep = "")
#   }else{
#     x
#   }
#   
# })
```

This tutorial is designed to provide some basic (hopefully review) background material on linear regression along with some practical examples.

Let's start with a review of the **idea** behind linear regression.  It is assumed that most of this is review and a number of terms will **not be formally defined** 

## Regression

Let's start with some interesting data with variables that might exhibit a reasonably linear relationships.  Using [Nitika's rpub](http://rpubs.com/Nitika/linearRegression_Airquality) as inspiration, our first example will utilize the built-in `airquality` dataset (see `help(topic="airquality")` for more details).  Let's first look at the relationship between the various variables using the `pairs()` function:

```{R}
data(airquality)
pairs(airquality)
```

When `Temp` is used as the **explanatory** variable  and `Ozone` as the **response** variable the resulting scatterplot shows a positive association:

```{r}
with(airquality,plot(Ozone~Temp,main="New York air quality May to September of 1973",xlab="Temperature (deg F)",ylab="Ozone (ppb)"))
```

Notice our use of `~` in the `plot()` function up above.  In **simple linear regression** R uses the function `lm()` and model notation which has the response variable on the left and the explanatory on the right:  `response~explanatory`.  Since the response variable is traditionally represented as the *y*-variable and the explanatory as the *x*-variable this can cause a bit of confusion when used in `plot()`.

### Linear models

We can use any line as a model for the relationship between `Temp` and `Ozone`, but some lines are better than others.  In the interactive graph below the "residual bars" represent the distance from each data-point to the proposed line.  The sum of the **squares** of these lengths is called the **sum of squares** for the line and represents a measure of **goodness of fit**:

```{r, echo=FALSE}
sliderInput("slope", "Slope:", min = -4, max = 4, value = 3,step = 0.25)
sliderInput("intercept", "Intercept:", min = -200, max = -100, value = -160)
checkboxInput("showResiduals","Show Residual Bars")
checkboxInput("showSS","Show Sum of Squares")
plotOutput("lmPlot")

```

```{r, context="server"}
output$lmPlot <- renderPlot({
  f=function(x){
    y.value<-input$intercept+input$slope*x #possibly a vector
    #if(length(y.value)==1){cat(y.value,"\n")}
    return(y.value)
  }
  pt.loc=function(x,y,f){ #returns 1 if (x,y) is above the line y=mx+b, -1 if below and 0 if upon
    results<-sign(y-f(x))
   # cat(paste(x,y,":",results,"\n"))              
    results
  }
     x=airquality$Temp
     y=airquality$Ozone
     p.y=f(x)
     y.l=0
     y.u=170
     x.l=50
     x.u=105
     # We need to ensure that lower two points on bounding box are below line 
     if(pt.loc(x.l,y.l,f)>0 | pt.loc(x.u,y.l,f)>0){
       y.l=min(f(x.l),f(x.u))
     }
     #Also must ensure that upper two points on bounding box are above line
     if(pt.loc(x.l,y.u,f)<1 | pt.loc(x.u,y.u,f)<1){
       y.u=max(f(x.l),f(x.u))
     }
     plot(y~x,
         main="New York air quality May to September of 1973",
         xlab="Temperature (deg F)",ylab="Ozone (ppb)",
         xlim=c(x.l,x.u),
         ylim=c(y.l,y.u)
    )
    abline(input$intercept,input$slope,lwd=2)
    if(input$showResiduals){
           segments(x,y,x,p.y,col="lightgray")
    }
    if(input$showSS){
      ss=sum((p.y-y)^2,na.rm=TRUE)
      ss=round(ss,1)
      text(60,150,paste("ss:",ss))
    }
})
```

 The **line of regression** or **line of best fit** is the *unique* line (we don't prove uniqueness in this class... but it is) that minimizes the sum of squares.  
 
We can view this line as a **model** with two **parameters**:  

* the **intercept**, and the 
* the **slope**.

The values that we calculate from the data are an **approximation** to the **true** values that express the actual linear relationship (as much as there is one) between the Temperature in New York in 1973 and the Ozone levels.  

We can use the linear model to:

* **Describe**, or to 
* **Predict**.

If the prediction occurs within the range of previously observed values (in this case between around 50 degrees F and 100 degrees F) then this is a form of **interpolation**.  If the prediction occurs outside the range then we are **extrapolating**.

In classic R we use the function `lm()` to produce the model.  Frequently we store the results in a variable and use the R command `summary()` for numeric information and `abline()` to add the line to the model:

```{r,echo=TRUE}
linear.model<-lm(data=airquality,Ozone~Temp)
summary(linear.model)
with(airquality,{
  plot(Ozone~Temp,
       xlab="Temp (degrees F)",
       ylab="Ozone (ppb)",
       main="New York air quality May to September of 1973")
  abline(linear.model)
})
```

### Linear Regression Exercise 

In the code exercise below perform the following actions:

* Determine how many `NA`'s exist in the `Ozone` variable of `airquality` and store in a variable called `n.NAs`
* Using the `lm()` function in R, find the line of best fit for the relationship between `Wind` and `Ozone`.  Store the model in a variable called `my.model`.
* Use `abline()` to add the line of best fit to the graph


```{r two-plus-two, exercise=TRUE,exercise.lines=6}
n.NAs<-#Your code here
with(airquality,{
  plot(Ozone~Wind,xlab="Wind Speed (mph)",ylab="Ozone (ppb)",main="New York air quality May to September of 1973")
  abline("stuff here")
})
```

## Testing the model

For any pair of variables it is always possible to make a line of best fit.  The utility of that line depends upon the actual relationship between the two variables.  For example data that follows a nice **quadratic** relationship (think *parabola*) will still have a line of best fit but it might not be particularly useful:

```{r}
set.seed(1000) #To ensure all "random" numbers are the same
n=200
x=runif(n,-0.5,2)
f=function(x){2*x^2-3*x+1}
y=f(x)+rnorm(n,0,0.1)
plot(x,y)
my.model<-lm(y~x)

abline(my.model)
```

One way to determine the appropriateness of the model is to look at the **residual plot** which plots the **residuals** against the **fitted values**.  Recall that for each data point the **observed value** is the data itself, the **fitted value** (also known as the **predicted value**) is value predicted for the response variable by plugging in the value of the explanatory variable into the model, and the **residual** is the difference between the observed value (the data) and the predicted (or fitted value)).  

In a good model there is no relationship between the value of the residual and the fitted value.  This is known as **homoscedasticity**.  In a bad model the residual exhibit **heteroscedasticity**.  

R will generate the residual plot as one of the diagnostic plots associated to a linear model, but we will use the following **convenience functions** to make it ourselves.  As you read the code below remember that `my.model` was generated in the previous code chunk:

```{r}
plot(fitted.values(my.model),residuals(my.model),main="Residual Plot of Parabola",xlab="fitted values",ylab="residuals")
```

We see that there is quite a strong relationship beteen the residual and the fitted value.

Let's look at another common problem.  Below, you will clearly see that the **point cloud** varies in "height" for different values of $x$.    This means that there is an association between the explanatory value (of $x$) and the **variance**.

```{r}
f=function(x){
  err=sapply(x,function(e){rnorm(1,0,abs(e))})
  #err=rnorm(length(x),0,0.2)
  #s=sign(err)
  2*x + 3 + err/4 #+s*abs(x^1.3)
}
y=f(x)
plot(x,y)
my.model2<-lm(y~x)
abline(my.model2)
plot(fitted.values(my.model2),residuals(my.model2),main="Residual Plot with non constant variance",xlab="fitted values",ylab="residuals")
```

Recall the ideas of **probability**, **distributions**, and **conditional probability**.  The key concepts that we care about are **random**, **expected value** (a measure of the **center** of a distribution), and **variance** ( a measure of the **spread** of a distribution).

The graph of a **function** must pass the **vertical line test**-- which means, graphically, that for any given input only one output is possible.  These point clouds frequently  do not satisfy those requirements... instead for any given value of $x$ (the explanatory variable) there may be **multiple** values in $y$ (the response variable).  For any **given** value of $x$ there is a **distribution** of values in $y$.  When the relationship between $x$ and $y$ is appropriate (which I'm not going to define carefully) the line of best fit is **really** expressing the **expected value** of the distribution of $y$ for a given value of $x$.  In other words:

$$
\mathbb{E}\textrm{(y|x)}
$$

The line-of-best-fit technology (linear regression) can always be applied and used to make predictions, but the standard techniques for assessing the liklihood of those predictions relies upon **constant variance** (what aka... **homoscedasticity**).  If those conditions are satisfied then confidence intervals for 

* **slope**
* **intercept**
* **$\mathbb{E}\textrm{(y|x)}$**
* **$\textrm{value of y given x}$**

are all theoretically accurate.
This occurs when the following relationship holds:

$$
\textrm{DATA = FIT + RESIDUAL}
$$
Where $\textrm{DATA}$ is the response variable.  $\textrm{FIT}$ is the **fitted value** predicted by the linera model and $\textrm{RESIDUAL}$ is the **residual**.  Furthermore, we need

$$
\textrm{RESIDUAL} \sim N(0,\sigma^2)
$$

In other words the residual (also known as the **error**) is normally distributed with a mean of 0 and variance of $\sigma^2$.

**When** those requirements are not satisifed then the values provided by functions like `summary()` may not be valid.

For a dense enough point cloud, One can pick a specific $x$ value (or very small range of $x$-values) and observe the corresponding $y$ distribution (these values are an estimate for the distribution of $y$ **given** $x$ (we use the notation $p(y|x)$).  The **expected value** (or **mean**) of $y$ given $x$ is not very useful if the distribution is skewed.

Packages such as `ggplot2` contain code to generate 

geom_smooth()
confint()

Even if the conditional distributions of $y$ are all relatively symmetric, the spread could vary from $x$ value to $x$ value-- resulting in heteroscedasticity.  Various techniques may be employed to transform the original data into a form that is closer to the requirements for a good linear model.  One of these **variance stabilizing** transformations is to take the $\log(y)}$ of a variables values, or $\sqrt{y}$

This is useful, but complicates interpretation.

### Explanatory Variables

The **linear** in linear regression refers to the relationship between  explanatory variables.  The variables, themselves, can be the result of non-linear processes.  A simple example would be to fit a quadratic curve.  **Dummy Variables** are another good example

EXPAND
### Measuring model fit

We have already seen that the line of best fit in Ordinarly Least Squares regression is the one that minimizes the Sum of Squares of the error.  Dividing this value by the number of observarionts $n$ for the model results in the MSE (Mean Sum of squares of error), which is, essentially, the variance of the error terms.  Also note that minimizes SSE also minimizes MSE (and vice versa).  It is related to the $r^2$ statistic.

$$
\begin{aligned}
r^2 &= 1 - \frac{\sum (y_i-\widehat{y_i})^2}{\sum{(y_i-\overline{y})^2}}\\
&=\frac{\sum{(y_i-\overline{y})}^2}{\sum{(y_i-\overline{y})}^2}- \frac{\sum (y_i-\widehat{y_i})^2}{\sum{(y_i-\overline{y})^2}}\\
&=\frac{\sum(y_i-\overline{y})^2-\sum (y_i-\widehat{y_i})^2}{\sum{(y_i-\overline{y})^2}}\\
\end{aligned}
$$
Notice that all the summations are just a factor of $\frac{1}{n-1}$ away from being sample variance equations.

Recall that if the errors are independent of the model:

$$
\begin{aligned}
\textrm{OBSERVED} &= \textrm{FIT} + \textrm{ERROR}\\
\textrm{VAR(OBSERVED)} &= \textrm{VAR(FIT)}+ \textrm{VAR(ERROR)}
\end{aligned}
$$

Notice further that:

$$
\textrm{VAR(FIT)} = \textrm{VAR(OBSERVED)}- \textrm{VAR(ERROR)}
$$

So that expression for $r^2$ when algebraically manipulated into the form of a single fraction ends up being:

$$

r^2 = \frac{\textrm{VAR(FIT)}}{\textrm{VAR(OBSERVED)}}
$$
In other words... $r^2$ really is the proportion of the variance in y (the $\textrm{OBSERVED}$ thta comes from the model $\textrm{VAR(FIT)}$)

We all recall that $r^2$ is a number that ranges from 0 to 1 and is **also** a good way of thinking about how close the data is to being on a straight line.

Also notice that under this defintiion it really does not matter how many explanatory variables are in the model.

R uses an **adjusted** $r^2$ to penalize a model with more parameters.

This adjusted-$r^2$ is one measure of whether or not a linear model is a good fit for the data.

We could also write the equation for $r^2$ as

$$
\begin{aligned}
r^2 &= \frac{\sum(y_i-\widehat{y_i})^2}{\sum(y_i-\overline{y})^2}\\
&=\frac{\textrm{Sum of Squares of Residuals}}{\textrm{Sum of Squares of Observations}}
\end{aligned}
$$

The R package `qpcr` has a useful variant of MSE called **Allen's PRESS** (Predicted Sum of Squares) and a varient of $r^2$ called  **p square**.  The idea for PRESS is to leave out each row in the table, use the remaining data to generate a model, then predict the response value for the omitted row.  The closer this value is to 0 ,the more internally consistent the data is with the model being employed.

The PRESS statistic, clearly depends rather strongly on the number of observations... we can standardize this value in a fashion that is similar to what we did for $r^2$.

Let $\widehat{y_i}$ be the predicted value for the $i$th observation and $y_i$ the true value.  If we let $\epsilon_i = y_i - \widehat{y_i}$ then, in analogy to $r^2$:

$$
p^2 = \frac{\sum\epsilon_i^2}{\sum(y_i-\overline{y})^2}
$$

https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/

Recall from earlier our linear model of air quality.  The `PRESS()` function that will calculate p-square does not seem to deal well with `NA`s so we'll want to remove them:

```{r}
library(qpcR)
reduced.air.quality=airquality[!is.na(airquality$Ozone),]
linear.model=lm(data=reduced.air.quality,Ozone~Temp)
summary(linear.model)
PRESS(linear.model) #Calculated the p square statistic
```

The closer to 0 the better this statistic.  Once the `NA`'s are removed the `qpcR` library will allow the PRESS and $p^2$ statistic to be produced for linear models as well as generalized linear models and even regression trees (all of which we will discusss later)

[Here](https://www.analyticbridge.datasciencecentral.com/profiles/blogs/use-press-not-r-squared-to-judge-predictive-power-of-regression) is an interesting, short, article on using the PRESS statistic as a measure of predictive power.  In particular, the example uses several models with similar $r^2$ values but rather different $p^2$ values... 


## Logistic Regression

With Generalized Linear Models (see the Mathematical Background for more details) we can attempt to make predictions about point-clouds that do not have a nice linear relationship.

One common situation arises when there is some sort of a yes/no question (the response) where the probability of "success" (getting a "yes") can change based upon the explanatory variables.  

In the simplest situation we have one continous explanatory variable, $x$, and one response variable $y$.  We use the following coding:

Result         |  Code
---------------|--------
$\textrm{no}$  | 0
$\textrm{yes}$ | 1

For any given observation $i$ there is a probability (based upon $x_i$) of $\pi_i$ for getting a 1 and $1-\pi_1$ of getting a 0.  All the observations with the same value of $x$ should have the **same** probability of getting a 0 (or a 1).  If we group all the observations with the same values for $x$ we can think of the data as having $m$ bins.  The distribution of $y$ in each bin will be a Binomial distribution.

Review the Logistic Regression section of MathBackground for more information on the details.  For now we will focus on being very practical.  


(heavily borrowing from https://towardsdatascience.com/simply-explained-logistic-regression-with-example-in-r-b919acb1d6b3)
I'd like to predict whether or not an applicant to UCLA is going to be admitted.  It seems reasonable that their high school GPA, their GRE score, and the rank (a proxy for quality) of their high school will influence the probability of them being admitted.  

Let's pull the data from UCLA:

```{r}
df <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
```

Do some basic summarization and graphical exploration of the data before proceeding.  I expect to see

* numeric summaries
* graphical summaries (single variables)
* graphical summaries (like side-by-side boxplot) comparing multiple variables

```{r ucla-example exercise}
df <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")

```

Let me remind you that the goal of regression, as we have been practicing it, is to find the **middle** of the $y$-values that would result... in other words our model should be predicting the *probablility* (for the given combination of explanatory values).  If all our model assumptions are satisfied the curve of best fit for this prediction is a logistic curve.

Our data has three explanatory variables and yes/no response.

We use the `glm()` function and must specify the keyword argument `family="binomial"'.  The resulting parameters tell us something about how the explanatory variables modify the "log odds".

Let's actually perform the logistic regression.  We will start by only using `gre` and `gpa` because 

```{r}
df <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
model=glm(data=df,family="binomial",admit~gre+gpa)
summary(model)
```

What are we seeing?

The equation is predicting the log odds:

$$
\log\left(\frac{p}{1-p}\right) = -4.93 + 0.003\ \textrm{gre} + 0.755\ \textrm{gpa}
$$

Note that $p=0$ or $p=1$ result in undefined expressions since $\log(0)$ is undefined and so is $\frac{1}{0}$.  

Exponentiating both sides produces an equation modeling the odds:

$$
\begin{aligned}
\frac{p}{1-p} &= e^{-4.93 + 0.003\ \textrm{gre} + 0.755\ \textrm{gpa}}\\
&= e^{-4.93}e^{0.003\ \textrm{gre}}e^{0.755\ \textrm{gpa}}\\
&=(0.007) 1.003^{\textrm{gre}}2.127^{\textrm{gpa}}
\end{aligned}
$$
Increasing $\textrm{gre}$ by 1 will multiply the RHS by 1.003.  In other words it increases the odds by a factor of 1.003.  The effect size might not be high... but a GRE score can vary quite a bit... increasing it by 100 increases the odds by $1.003^{100}=1.349$.  GPA on the other hand, which has a much more limited range, has a larger effect on a **per-point** basis (whether or not such a distinction is important I will leave for you to consider).

Notice that $1+odds = \frac{1}{1-p}$
$$
\begin{aligned}
\frac{odds}{1+odds}&= \frac{p}{1-p}\frac{1}{1+\frac{p}{1-p}}\\
&=\frac{p}{1-p}\frac{1}{\frac{1-p}{1-p}+\frac{p}{1-p}}\\
&=\frac{p}{1-p}\frac{1}{\frac{1}{1-p}}\\
&=\frac{p}{1-p}\frac{p-1}{1}\\
&=p
\end{aligned}
$$

So although we can turn information about the odds into information about probability... a straightforward interpretation is still a bit problematics.

Let's train our intuition a bit by looking at just gpa:

```{r}
plot(df$gpa,jitter(df$admit,0.1),col=rgb(0,0,0,0.4),pch=".",cex=3)
```

The y-axis is quite boring-- just 0/1.  But by using some jitter and transparency to deal with overplotting we see that an increase in GPA really does seem to improve the probability of being accepted.

The model of this is

```{r}
onlygpa=glm(data=df,admit~gpa,family="binomial")
summary(onlygpa)
```

So

$$
\begin{aligned}
odds&=(0.0128)2.8608^{\textrm{gpa}}\\
\frac{odds}{1+odds} &= \frac{(0.0128)2.8608^{\textrm{gpa}}}{1+(0.0128)2.8608^{\textrm{gpa}}}\\
p&=\frac{1}{(78.0686)2.8608^{-\textrm{gpa}} +1}
\end{aligned}
$$
```{r}
plot(df$gpa,jitter(df$admit,0.1),col=rgb(0,0,0,0.4),pch=".",cex=3,ylab="")
.f=function(x){
  1/(1+78.0686*2.8608^(-x))
}
curve(.f(x),-2,4,add=TRUE)
```

That's not too shabby.  If you have a 4.0 GPA your probability of acceptance is about 46% and that's what the model predicts.

```{r}
table(df$admit[df$gpa==4])
13/28
.f(4)
```

**Exercise:** Repeat the process above but use $\textrm{gre}$ as the explanatory variable.  Produce the model and graph the logistic curve for $p$ with explanatory variable $\textrm{gre}$.  Check the prediction for $\textrm{gre}=800$ against the actual sample probability.

```{r gre-exercise exercise}
```

NOTE: Up above I solved the equation for $p$ algebraically but sometimes it is easier to just let the computer do it for us.  The expression `fitted.value(model)` will generate the fitted values.  I will redo the logistic regression using $\textrm{gpa}$ and $\textrm{gre}$, then produce a formula that will convert the explanatory values to the fitted values.  We can then compare the output of `fitted.values()` to our results.  Notice that we have **two** explanatory variables... so we'll need to be a bit fancier in our graphing.  I will use the graphing package `plotly`

```{r}
library(plotly)
model = glm(data=df,admit~gpa+gre,family="binomial")
fv=fitted.values(model)
plot_ly(data=df,size=0.2) %>% add_markers(x=~gpa,y=~gre,z=~fv) %>% layout(scene = list(xaxis = list(title = 'GPA'), 
                    yaxis = list(title = 'GRE'),
                    zaxis = list(title = 'Probability'))) #%>% add_segments()

```
Now let's see if we correctly understand the equation.  First we get the coefficients from the model:

```{r}
coefficients(model)
```

These three values fit into the equation:

$$
\log\left(\frac{p}{1-p}\right)=-4.944 + 0.755\textrm{ gpa}+0.003\textrm{ gre}
$$

The left hand side is the *log odds* (aka logit).  Let's capture this in an equation.  Make sure you understand how my expression matches the RHS of the equation.

```{r}
log.odds=function(x){ #Assume variable model is defined
  sum(coefficients(model)*c(1,as.numeric(x[c("gpa","gre")])))
}
```

Now let's turn this into the actual probabilities:

```{r}
logistic=function(lo){
  1/(exp(-1*log.odds(lo))+1)
}
```

We can use `apply` to apply this function to every row in the data.frame to compare our values to the fitted values:

```{r}
our.values=apply(df,1,logistic)
summary(our.values-fv)
```

Notice how close our values are... just a few rounding issues (`-1.110e-16` and `1.110e-16` are VERY close to 0).

So... success... we understand how to do this.

Now **you** do the same thing, but with the full model.  Be sure to put your summary into a variable called 
`my.summary`

```{r checking-logistic-equation exercise}
```


://www.statisticalassociates.com/logistic10.htm

http://www.statisticalassociates.com/booklist.htm

```{r simulation-exercise exericise}

```

Look at the first few lines of the table:

```{r,results="asis"}
library(knitr)
library(kableExtra)
library(magrittr)
rf=head(df,5)
rf%>%kable()%>%kable_styling()%>%row_spec(which(rf$admit==1), background = "#CAEFCA",bold=T)
```

I have highlighted the rows that were associated to admitted individuals.  

Let's build a full model (using `rank` as a factor()) and predict "admit" and make some predictions about admittance using our model.  The idea is that we let the explanatory variables predict the probability.  

Some people use logistic regression for classification. [There are good arguments against this](https://www.fharrell.com/post/classification/) (you should follow and read that link).  

But, that said, we could try it anyway... we need to make some decision... will we call a row isn the table a "yes" or a "no".  One approach would be to say that anything *at or above* 0.5 counts as a yes(1).  And anything below counts as a no (0).

Let's build the full model and make our predictions:
```{r}
full.model=glm(data=df,admit~gpa+gre+factor(rank),family="binomial")
fv=fitted.values(full.model)
predicted.cat=ifelse(fv>=0.5,1,0)
table(df$admit,predicted.cat)
```

The rows are the actual values, and the cols are the predicted values.  This table is known as the **confusion matrix** (because the off-diagonals measure the mistakes)

You can see that for 254 cases our algorithm predicted "no" and that was the result.  In 30 cases, our algorithm predicted "yes" and that was the result.  In 19 cases our algorithm predicted "yes" when the true situations was "no".  And the most extreme situation mistakes occurred when we predicted "no", but the true situation wasy "yes"

Each of the squares in the table have names:

* **True Positives**:  The observed and predicted value were both 1 (30 of these for us)
* **True Negatives**:  The observed and predicted value were both 0 (254 of these for us)
* **False Positives**: Prediction was "yes", but observed was "no" (19 of these for us)
* **False Negatives**: Prediction was "no", but observed was "yes" (97 of these for us)

False Positives are also known as **type I errors** and False Negatives are also known as **type II errors**.  (Think about that for a bit in the context of what you learned in Introduction to Statistics and Hypothesis Tests)


Make the model in the console use $\textrm{gpa}$ and $\textrm{gre}$ as the predictors.  Use 0.5 as the prediction threshold.  Answer the following questions

```{r ML-quiz,echo=FALSE}
quiz(
  question("The number of true positives",
    answer("263"),
    answer("10"),
    answer("118"),
    answer("9", correct = TRUE)
  ),
    question("The number of false positives",
    answer("263"),
    answer("10", correct = TRUE),
    answer("118"),
    answer("9")
  ),
    question("The number of true negatives",
    answer("263", correct = TRUE),
    answer("10"),
    answer("118"),
    answer("9")
  ),
    question("The number of false negatives",
    answer("263"),
    answer("10"),
    answer("118", correct = TRUE),
    answer("9")
  )
)
```

It's not too surprising that the number of false negatives is the highest category.  If you **ignored every predictor** your best guess would be "no".  And this simple algorithm woudl give you the proper predictor 68.25% of the time!  

Let's call this the **accuracy**.  In other words the number of predictions that were correct.  

$$
\textrm{Accuracy} = \frac{\textrm{True Positives}+\textrm{True Negatives}}{\textrm{Total}}
$$

So... in our example the "always guess *no*" approach has an accuracy of over 68%.  Notice that the logistic regression approach from your last exercise (using a threshold of 0.5) has an accuracy of $\frac{263+9}{400} = 68%$.  Hmmm... that wasn't very satisfying was it?  Even including the `rank` (as a factor) only changes the accuracy to 71%

There does seem to be something fundamentally different about the two strategies... The "always say no" approach will never yield a false positive (nor a true positive).  The logistic regression prediction approach is a bit more balanced.  

Let's see what effect changing our threshold has.  Let's go back to the full model and make an accuracy curve:

```{r}
full.model=glm(data=df,admit~gpa+gre+factor(rank),family="binomial")

rc=function(threshold,m){
  tmp=ifelse(fitted.values(m)>=threshold,1,0)
  if(all(tmp==0)){return(sum(df$admit==0)/nrow(df))}
  if(all(tmp==1)){return(sum(df$admit==1)/nrow(df))}
  tt=table(tmp,df$admit)
  (tt[1,1]+tt[2,2])/sum(tt)
}
rcv=Vectorize(rc,vectorize.args="threshold")
results=curve(rcv(x,full.model),0,1)
results$x[which.max(results$y)]
results$y[which.max(results$y)]

```

We can get up to 71.5% if we use 49% as a threshold... lets' look a bit closer:

```{r}
results=curve(rcv(x,full.model),0.45,0.55)
results$x[which.max(results$y)]
results$y[which.max(results$y)]
```

A very minor improvement by changing the threshold to 48.6%... still that's pretty strange.

Depending upon your purposes you might be willing to have fewer accurate predictions as long as you have fewer false positives... take, for example, a spam filter (which we'll discuss below in the context of Naive Bayes and the bag of words model).

It is far better for some real spam slip into a persons inbox then for a real communication to be classified as spam and filtered.  In that situation, we might use logistic regression and set a threshold of 0.9.

in that situation we would be far more concerned with the **precision**-- that's the percentage of things classified as "yes" that are correct:

$$
\textrm{Precision} = \frac{\textrm{True Positives}}{\textrm{True Positives}+\textrm{False Positives}}
$$
Another useful measure is **recall** (also known as **sensitivity** also known as **true positive rate**)- this measures the percentage of the **actual positives** that  were labeled correctly

$$
\textrm{Recall} = \frac{\textrm{True Positives}}{\textrm{True Positives}+\textrm{False Negatives}}
$$

Read that a couple of times (it always confuses me the first time I review the material).  THe true positives are the cases labeled positive that **are** positive.  The False negatives are the mislabeled **actual positives**.  So the recall (or sensitivity) is a measure of how many of the positives are identified (ignoring false positives).

It's easy to get a high recall-- just adopt the "always say yes" strategy.  Then EVERYTHING is a positive... and so the recall is always 100%.  

We also have **specificity** (also known as the **true negative rate**)

$$
\textrm{Specificity} = \frac{\textrm{True Negatives}}{\textrm{True Negatives}+\textrm{False Positives}}
$$

Let's think of the possible results that could occur for test that tries to detect a disease:

metric         |    interpretation
---------------|--------------------
Accuracy       | P(test result is correct)
Sensitivity    | P(result is positive | patient has disease)
Specificity    | P(result is negative | patient does not have disease)

There's one other common measure you might encounter... It's called $F1$ and it tries to balance the true positive rate and the true negative rate.

$$
\textrm{F1} = 2\left(\frac{\textrm{Precision}*\textrm{Recall}}{\textrm{Precision}+\textrm{Recall}}\right)
$$
Notice that if our accuracy was **perfect** then $\textrm{Precision}$ and $\textrm{Recall}$ would both be 1.  In this situation $F1$ would be 1.  If either $\textrm{Precision}$ and $\textrm{Recall}$ is 0, then the numerator is 0 and the denominator is **not 0** (they can't both be 0 at the same time).  This is actually (after some algebraic manipualtion) the **harmonic mean** of $\textrm{precision}$ and $\textrm{recall}$.

## Decision Trees

Perhaps a better way to make a decision is to use something more suited to classification... such as a decision tree read [this article](https://medium.com/analytics-vidhya/a-guide-to-machine-learning-in-r-for-beginners-decision-trees-c24dfd490abb) now.  

We already have the data in the `df` dataframe:

```{r}
# Load CART packages
library(rpart)
library(rpart.plot)

df$rank=factor(df$rank) # Best for rank to be a factor
admit.tree=rpart(admit ~ gre + gpa+rank, data=df,method="class")
prp(admit.tree)
```

So, given a row in the table we follow the directions by answering a series of yes/no questions:

Let's use the first row as an example.

1. $\textrm{gpa}<3.4$ NO (go right)
1. $\textrm{rank}$ is 2, 3, or 4.  YES (go left)
1. Prediction is 0

Just like the results from the logistic regression we can use the model to make a prediction.

First look at the first few predictions:

```{r}
tree.p=predict(admit.tree)
head(tree.p)
```

You can see that the tree puts  70% chance that row 1 is a 0 and a 30% chance that it is a 1.  We want to pick the larger of the two values.  We can, fairly easily, look at the column for 1.  If it is higher than 0.5 then we'll choose 1 (since each row must add to 1).  I choose higher than 0.5 instead of higher than *or equal* to 0.5 since there are more 0's... so if we don't know any better, might as well choose the more common one.  Note that the innermost expression evaluates to `TRUE` and `FALSE`, but `as.numeric` turns `FALSE` to 0 and `TRUE` to 1:

Let's make the confusion matrix for this classification:

```{r}
pv=predict(admit.tree,type="class")
table(df$admit,pv)
```

Our accuracy is $\frac{249+54}{400} = 0.7575$.  That's better than before.  Also our Type I and Type II error rates are more balanced.

Notice that **it is impossible** to guess everything correctly.  Rows 15 and 166 on the table are identical in all respects **except** the value of *admit*:
```{r results="asis"}
kable(df[c(15,166),]) %>%kable_styling()
```

Now... MOST of the combinations of values are distinct... which means that one **could**, produce a very exhaustive list of special cases to make VERY accurate predictions... HOWEVER that would only work on the specific data-set and would be extremely unlikely to transfer well to new data.  This problem is known as **over-fitting**.

However... look at the tree again and think about what it's saying...
The first thing it looks at is a student's gpa.  Is it low (below 3.4) or high?
That's the first devision.  If the GPA is low than a student who does not come from a well-ranked school is weeded out.  If they DO come from a higher ranked school then they'd better have a pretty impressive GRE score.  A score of less than 730 rules them out... otherwise they're in. (probabilistically speaking of course)

On the other hand.. if the GPA is higher and the school is top ranked... then the student is accepted if the school is low ranked 3 or 4 the student is rejected.  After this the GPA is taken into consideration.  This one gets a bit... *weird*  If your GPA iS between 3.4 and 3.5 then you're accepted.  If your GPA is between 3.5 and 3.7 you are rejected.  Otherwise your GRE is used to determine your acceptance.

Let's add `extra=1` to the `prp` function.  This will add the number of rows correctly (and incorrectly) classified:

```{r}
prp(admit.tree,extra=1)
```

This provides us a better sense from whence the errors in prediction are arising.... first off-- notice that no matter what occurs at the first split-- a rank 3 or rank 4 school is given a best guess of 0... so in that subset... without taking anything else into consideration just saying predicting "reject" will be correct almost 79% of the time... So that's a pretty good indicator.  It's not that the tree couldn't decide to be more aggressive in its branching... but that defeats part of the purpose-- the goal is to identify *trends*-- simplifying patterns that tell us something meaningful about the overall system-- not to blindly pursue accuracy at the cost of sensibiity.

The `summary` provides another way to view the rules.  Note that some nodes may be "collapsed" and not appear in the graphed tree:

<div style="max-height: 300px;float: left;width: 100%;overflow-y: auto;">
```{r}
summary(admit.tree)
```
</div>

At each stage of the game the decision tree is trying to **partition** the data into two sets based upon some "rule.  Each set is given a "prediction".  If the split can successfully improve discrimination beyond a certain threshold then it is added.  There are usually restrictions on both **depth** and **bin size**.  For example, the process, typically, stops trying when it reaches a depth of 30, or when the partition under consideration has 5 or fewer observations.  This generates a tree that is, almost certianly, too complicated (recall what we said about *over fitting* earlier).  

The second step is to **trim** the tree.  The `rpart` function does some of this automatically-- you might wish to do some more.  The `prune()` function allows you to do this

Read [this DZone article on decision trees](https://dzone.com/articles/how-to-create-a-perfect-decision-tree)

There are a number of decisions to make about both what the model is saying, as well as how good of a job it is doing.

function                         |   explanation
---------------------------------|-------------
`rpart(...,method="class")`      | Build the classification tree.  Uses conventions similar to `lm()`
`prp(tree)`                      | Makes a graph of tree (from `rpart.plot` package)
`summary(tree)`                    | Shows the rules, the call, the printcp table, and variable importance.
`plotcp(tree)`                      | Produce the complexity plot-
`printcp(tree)                      | table of the complexity plot
`print(tree)`                       | essentially sumary
`PRESS(tree)`                       |should work but doesn't                       
`prune(tree)`                       | prune the tree   

In general, we assess by making a confusion matrix via `predict()` and `table()`.  More particularly, `rpart` performs cross-validation and this information is accessed graphically via `plotcp()` and in table form as `printcp()`.  It is also included in `summary()`

```
The x-error is the cross-validation error (rpart has built-in cross validation). You use the 3 columns, rel_error, xerror and xstd together to help you choose where to prune the tree.

Each row represents a different height of the tree. In general, more levels in the tree mean that it has lower classification error on the training. However, you run the risk of overfitting. Often, the cross-validation error will actually grow as the tree gets more levels (at least, after the 'optimal' level).

A rule of thumb is to choose the lowest level where the rel_error + xstd < xerror.

If you run plotcp on your output it will also show you the optimal place to prune the tree.
```
From https://stackoverflow.com/questions/29197213/what-is-the-difference-between-rel-error-and-x-error-in-a-rpart-decision-tree?rq=1

```
5

I would like to add some info to @Harold Ship's answer. Actually, there are three ways to select the optimal cp value for pruning:

Use the first level (i.e. least nsplit) with minimum xerror. The first level only kicks in when there are multiple level having same, minimum xerror. This is the most common used method.

Use the first level where xerror falls into the ±1 xstd range of min(xerror), i.e., xerror < min(xerror) + xstd, the level whose xerror is at or below horizontal line. This method takes into account the variability of xerror resulting from cross-validation.

Note: rel_error should NOT be used in pruning.

(A rarely used method) Use the first level where xerror ± xstd overlaps with min(xerror) ± xstd. i.e., the level whose lower limit is at or below horizontal line.
```

**Exercise:** Now read [this article](http://uc-r.github.io/regression_trees) and reproduce their code (no copy-pasting allowed!) which discusses using regression trees to categorize more than just a binary response.  It's the same basic idea, but the prediction is for more than just 0 or 1.  Notice that the `rpart()` function is using `method="anova"` rather than `method="class"`.

Pay special attention to the discussion around 

* The PRESS Statistic
* Relative Error
* Cross-Validation X error

`plotcp(model)`, `printcp(model)`

DEFINE:
CROSS-VALIDATION ERROR
PRESS STATISTIC
MSE
```{r regression-exercise-external exercise}
library(rsample)
library(dplyr)
library(rpart)
library(rpart.plot)
library(ipred)
library(caret)

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(),prop=0.7)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)

m1 <- rpart(
  formula=Sale_Price ~ .,
  data   =ames_train,
  method ="anova"
)

rpart.plot(m1) #plot the tree
plotcp(m1)     #plot complexity parameter table
```

## Random Forests

This is a pretty small data set.  When the data set is larger there may be conflicting choices, more noise, and other issues that make producing the optimal decision tree problematic.  One approach is to pick a random subset of the data (rows **and** columns) and build a tree from that subset.  Repeat this process many, many times.  The rows are selected **with replacement** (This is known as **bootstrapping**).  

I want to reiterate a few details about the **random subset** idea.  For random forests this is more than just selecting random rows with replacement.  The explanatory variables are ALSO selected at random.  As the many random trees are produced, the various explanatory variables involved in the rules are monitored and a running tally of **the most important variables** are produced.

There are many **aggregation techniques** that do something similar.  Bootstrap aggregation (**bagging**) is one such example.  However, randomly producing trees can generate many trees that are too similar.  This reduces the effectiveness of the technique.  The **random forest** idea 

**EXERCISE:** Read the [following tutorial from uc-r](https://uc-r.github.io/random_forests) and reproduce their example. Do **NOT** copy and paste.

```{r random-forest-exercise exercise}
library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # an extremely fast java-based platform
library(AmesHousing)
set.seed(123)
ames_split<-initial_split(AmesHousing::make_ames(),prop=0.7)
ames_train <- training(ames_split)

```

POTENTIALLY Read [Leo Breiman and ADele Cutler's manual on Random Forests](https://www.stat.berkeley.edu/~breiman/RandomForests/).  

```{r}
#cars<-read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data")
```
## Boosted Regression
The package `gbm` will perform boosted regression and the summary provides a nice summary of the relative influence of the explanatory variables:

```{r}
library(gbm)
test=gbm(data=df,admit~gre+gpa+rank)
summary(test)
```

## Regression Trees

Rather than just attempting to classify, the partitioning technique can *also* break the explanatory-data space into subspaces... each with their own regression line... think of it as a decision tree whose leaves result in a data set and a regression on that set.

NOTE:  
DISCUSS DIFFERENCE BETWEEN CLASSIFICATION TREE AND REGRESSION TREE
MAKE SOME PLOTS
ADD SOME EXERCISES
DRAWBACKS AND SHORTCOMINGS OF THIS APPROACH (RANDOM NATURE OF CUTS TOO)

### Exercise with Hint

*Here's an exercise where the chunk is pre-evaulated via the `exercise.eval` option (so the user can see the default output we'd like them to customize). We also add a "hint" to the correct solution via the chunk immediate below labeled `print-limit-hint`.*

Modify the following code to limit the number of rows printed to 5:

```{r print-limit, exercise=TRUE, exercise.eval=TRUE}
mtcars
```

```{r print-limit-hint}
head(mtcars)
```

### Quiz

*You can include any number of single or multiple choice questions as a quiz. Use the `question` function to define a question and the `quiz` function for grouping multiple questions together.*

Some questions to verify that you understand the purposes of various base and recommended R packages:

```{r quiz}
quiz(
  question("Which package contains functions for installing other R packages?",
    answer("base"),
    answer("tools"),
    answer("utils", correct = TRUE),
    answer("codetools")
  ),
  question("Which of the R packages listed below are used to create plots?",
    answer("lattice", correct = TRUE),
    answer("tools"),
    answer("stats"),
    answer("grid", correct = TRUE)
  )
)
```


## Non parameteric regression

(Borrowing heavily from Chapter 18 of John Fox's Applied Regression Analysis and Generalized Liner Models)

The techniques we have discussed earlier are all based on an underlying model whose exact from is determined by numeric **parameters**.  The process of regression determines reasonable values for these parameters.  There are other techniques that do not 

One technique that does not rely upon an underlying parametric model for the data is known as **local linear regression** (or **loess**).  As with most non-parametric forms of regressin 