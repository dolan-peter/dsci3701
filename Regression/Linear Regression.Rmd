---
title: "Regression I Tutorial"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(knitr)
library(kableExtra)
library(magrittr)
knitr::opts_chunk$set(echo = FALSE)
# knitr::knit_hooks$set(output = function(x, options){
#   if(!is.null(options$max_height)){
#     paste('<pre style = "max-height:',
#           options$max_height, 
#           ';float: left; width: 910px; overflow-y: auto;">',
#           x,
#           "</pre>",
#           sep = "")
#   }else{
#     x
#   }
#   
# })
```

## Regression

This tutorial is designed to provide some basic (hopefully review) background material on linear regression along with some practical examples.

Let's start with a review of the **idea** behind linear regression.  It is assumed that most of this is review and a number of terms will **not be formally defined**.

Let's start with some interesting data with variables that might exhibit a reasonably linear relationships.  Using [Nitika's rpub](http://rpubs.com/Nitika/linearRegression_Airquality) as inspiration, our first example will utilize the built-in `airquality` dataset (see `help(topic="airquality")` for more details).  Let's first look at the relationship between the various variables using the `pairs()` function:

```{R}
data(airquality)
pairs(airquality)
```

When `Temp` is used as the **explanatory** variable  and `Ozone` as the **response** variable the resulting scatterplot shows a positive association:

```{r,echo=TRUE}
with(airquality,plot(Ozone~Temp,main="New York air quality May to September of 1973",xlab="Temperature (deg F)",ylab="Ozone (ppb)"))
```

Notice our use of `~` in the `plot()` function up above.  In **simple linear regression** R uses the function `lm()` and model notation which has the response variable on the left and the explanatory on the right:  `response~explanatory`.  Since the response variable is traditionally represented as the *y*-variable and the explanatory as the *x*-variable this can cause a bit of confusion when used in `plot()`.

### Linear models

We can use any line as a model for the relationship between `Temp` and `Ozone`, but some lines are better than others.  In the interactive graph below the "residual bars" represent the distance from each data-point to the proposed line.  The sum of the **squares** of these lengths is called the **sum of squares** for the line and represents a measure of **goodness of fit**:

```{r, echo=FALSE}
sliderInput("slope", "Slope:", min = -4, max = 4, value = 3,step = 0.25)
sliderInput("intercept", "Intercept:", min = -200, max = -100, value = -160)
checkboxInput("showResiduals","Show Residual Bars")
checkboxInput("showSS","Show Sum of Squares")
plotOutput("lmPlot")

```

```{r, context="server"}
output$lmPlot <- renderPlot({
  f=function(x){
    y.value<-input$intercept+input$slope*x #possibly a vector
    #if(length(y.value)==1){cat(y.value,"\n")}
    return(y.value)
  }
  pt.loc=function(x,y,f){ #returns 1 if (x,y) is above the line y=mx+b, -1 if below and 0 if upon
    results<-sign(y-f(x))
   # cat(paste(x,y,":",results,"\n"))              
    results
  }
     x=airquality$Temp
     y=airquality$Ozone
     p.y=f(x)
     y.l=0
     y.u=170
     x.l=50
     x.u=105
     # We need to ensure that lower two points on bounding box are below line 
     if(pt.loc(x.l,y.l,f)>0 | pt.loc(x.u,y.l,f)>0){
       y.l=min(f(x.l),f(x.u))
     }
     #Also must ensure that upper two points on bounding box are above line
     if(pt.loc(x.l,y.u,f)<1 | pt.loc(x.u,y.u,f)<1){
       y.u=max(f(x.l),f(x.u))
     }
     plot(y~x,
         main="New York air quality May to September of 1973",
         xlab="Temperature (deg F)",ylab="Ozone (ppb)",
         xlim=c(x.l,x.u),
         ylim=c(y.l,y.u)
    )
    abline(input$intercept,input$slope,lwd=2)
    if(input$showResiduals){
           segments(x,y,x,p.y,col="lightgray")
    }
    if(input$showSS){
      ss=sum((p.y-y)^2,na.rm=TRUE)
      ss=round(ss,1)
      text(60,150,paste("ss:",ss))
    }
})
```

 The **line of regression** or **line of best fit** is the *unique* line (we don't prove uniqueness in this class... but it is) that minimizes the sum of squares.  
 
We can view this line as a **model** with two **parameters**:  

* the **intercept**, and the 
* the **slope**.

The values that we calculate from the data are an **approximation** to the **true** values that express the actual linear relationship (as much as there is one) between the Temperature in New York in 1973 and the Ozone levels.  

We can use the linear model to:

* **Describe**, or to 
* **Predict**.

If the prediction occurs within the range of previously observed values (in this case between around 50 degrees F and 100 degrees F) then this is a form of **interpolation**.  If the prediction occurs outside the range then we are **extrapolating**.

In classic R we use the function `lm()` to produce the model.  Frequently we store the results in a variable and use the R command `summary()` for numeric information and `abline()` to add the line to the model:

```{r,echo=TRUE}
linear.model<-lm(data=airquality,Ozone~Temp)
summary(linear.model)
with(airquality,{
  plot(Ozone~Temp,
       xlab="Temp (degrees F)",
       ylab="Ozone (ppb)",
       main="New York air quality May to September of 1973")
  abline(linear.model)
})
```

### Linear Regression Exercise 

In the code exercise below perform the following actions:

* Determine how many `NA`'s exist in the `Ozone` variable of `airquality` and store in a variable called `n.NAs`
* Using the `lm()` function in R, find the line of best fit for the relationship between `Wind` and `Ozone`.  Store the model in a variable called `my.model`.
* Use `abline()` to add the line of best fit to the graph


```{r basicRegression, exercise=TRUE,exercise.lines=6}
n.NAs<-#Your code here
with(airquality,{
  plot(Ozone~Wind,xlab="Wind Speed (mph)",ylab="Ozone (ppb)",main="New York air quality May to September of 1973")
  abline("stuff here")
})
```

## Testing the model

For any pair of variables it is always possible to make a line of best fit.  The utility of that line depends upon the actual relationship between the two variables.  For example data that follows a nice **quadratic** relationship (think *parabola*) will still have a line of best fit but it might not be particularly useful:

```{r}
set.seed(1000) #To ensure all "random" numbers are the same
n=200
x=runif(n,-0.5,2)
f=function(x){2*x^2-3*x+1}
y=f(x)+rnorm(n,0,0.1)
plot(x,y)
my.model<-lm(y~x)

abline(my.model)
```

One way to determine the appropriateness of the model is to look at the **residual plot** which plots the **residuals** against the **fitted values**.  Recall that for each data point the **observed value** is the data itself, the **fitted value** (also known as the **predicted value**) is value predicted for the response variable by plugging in the value of the explanatory variable into the model, and the **residual** is the difference between the observed value (the data) and the predicted (or fitted value)).  

In a good model there is no relationship between the value of the residual and the fitted value.  This is known as **homoscedasticity**.  In a bad model the residual exhibit **heteroscedasticity**.  

R will generate the residual plot as one of the diagnostic plots associated to a linear model, but we will use the following **convenience functions** to make it ourselves.  As you read the code below remember that `my.model` was generated in the previous code chunk:

```{r}
plot(fitted.values(my.model),residuals(my.model),main="Residual Plot of Parabola",xlab="fitted values",ylab="residuals")
```

We see that there is quite a strong relationship beteen the residual and the fitted value.

Let's look at another common problem.  Below, you will clearly see that the **point cloud** varies in "height" for different values of $x$.    This means that there is an association between the explanatory value (of $x$) and the **variance**.

```{r}
f=function(x){
  err=sapply(x,function(e){rnorm(1,0,abs(e))})
  #err=rnorm(length(x),0,0.2)
  #s=sign(err)
  2*x + 3 + err/4 #+s*abs(x^1.3)
}
y=f(x)
plot(x,y)
my.model2<-lm(y~x)
abline(my.model2)
plot(fitted.values(my.model2),residuals(my.model2),main="Residual Plot with non constant variance",xlab="fitted values",ylab="residuals")
```

Recall the ideas of **probability**, **distributions**, and **conditional probability**.  The key concepts that we care about are **random**, **expected value** (a measure of the **center** of a distribution), and **variance** ( a measure of the **spread** of a distribution).

The graph of a **function** must pass the **vertical line test**-- which means, graphically, that for any given input only one output is possible.  These point clouds frequently  do not satisfy those requirements... instead for any given value of $x$ (the explanatory variable) there may be **multiple** values in $y$ (the response variable).  For any **given** value of $x$ there is a **distribution** of values in $y$.  When the relationship between $x$ and $y$ is appropriate (which I'm not going to define carefully) the line of best fit is **really** expressing the **expected value** of the distribution of $y$ for a given value of $x$.  In other words:

$$
\mathbb{E}\textrm{(y|x)}
$$

The line-of-best-fit technology (linear regression) can always be applied and used to make predictions, but the standard techniques for assessing the liklihood of those predictions relies upon **constant variance** (what aka... **homoscedasticity**).  If those conditions are satisfied then confidence intervals for 

* **slope**
* **intercept**
* **$\mathbb{E}\textrm{(y|x)}$**
* **$\textrm{value of y given x}$**

are all theoretically accurate.
This occurs when the following relationship holds:

$$
\textrm{DATA = FIT + RESIDUAL}
$$
Where $\textrm{DATA}$ is the response variable.  $\textrm{FIT}$ is the **fitted value** predicted by the linera model and $\textrm{RESIDUAL}$ is the **residual**.  Furthermore, we need

$$
\textrm{RESIDUAL} \sim N(0,\sigma^2)
$$

In other words the residual (also known as the **error**) is normally distributed with a mean of 0 and variance of $\sigma^2$.

**When** those requirements are not satisifed then the values provided by functions like `summary()` may not be valid.

For a dense enough point cloud, One can pick a specific $x$ value (or very small range of $x$-values) and observe the corresponding $y$ distribution (these values are an estimate for the distribution of $y$ **given** $x$ (we use the notation $p(y|x)$).  The **expected value** (or **mean**) of $y$ given $x$ is not very useful if the distribution is skewed.

Packages such as `ggplot2` contain code to generate these localized regression.  From example `geom_smooth()`.

You can also extract the confidence intervals for various parameters using `confint()`.

<!-- add examples -->

Even if the conditional distributions of $y$ are all relatively symmetric, the spread could vary from $x$ value to $x$ value-- resulting in heteroscedasticity.  Various techniques may be employed to transform the original data into a form that is closer to the requirements for a good linear model.  One of these **variance stabilizing** transformations is to take the $\log(y)$ of a variables values, or $\sqrt{y}$

This is useful, but complicates interpretation.

### Explanatory Variables

The **linear** in linear regression refers to the relationship between  explanatory variables.  The variables, themselves, can be the result of non-linear processes.  A simple example would be to fit a quadratic curve.  **Dummy Variables** are another good example

<!--EXPAND-->
### Measuring model fit

We have already seen that the line of best fit in Ordinarly Least Squares regression is the one that minimizes the Sum of Squares of the error.  Dividing this value by the number of observarionts $n$ for the model results in the MSE (Mean Sum of squares of error), which is, essentially, the variance of the error terms.  Also note that minimizes SSE also minimizes MSE (and vice versa).  It is related to the $r^2$ statistic.

$$
\begin{aligned}
r^2 &= 1 - \frac{\sum (y_i-\widehat{y_i})^2}{\sum{(y_i-\overline{y})^2}}\\
&=\frac{\sum{(y_i-\overline{y})}^2}{\sum{(y_i-\overline{y})}^2}- \frac{\sum (y_i-\widehat{y_i})^2}{\sum{(y_i-\overline{y})^2}}\\
&=\frac{\sum(y_i-\overline{y})^2-\sum (y_i-\widehat{y_i})^2}{\sum{(y_i-\overline{y})^2}}\\
\end{aligned}
$$
Notice that all the summations are just a factor of $\frac{1}{n-1}$ away from being sample variance equations.

Recall that if the errors are independent of the model:

$$
\begin{aligned}
\textrm{OBSERVED} &= \textrm{FIT} + \textrm{ERROR}\\
\textrm{VAR(OBSERVED)} &= \textrm{VAR(FIT)}+ \textrm{VAR(ERROR)}
\end{aligned}
$$

Notice further that:

$$
\textrm{VAR(FIT)} = \textrm{VAR(OBSERVED)}- \textrm{VAR(ERROR)}
$$

So that expression for $r^2$ when algebraically manipulated into the form of a single fraction ends up being:

$$
r^2 = \frac{\textrm{VAR(FIT)}}{\textrm{VAR(OBSERVED)}}
$$
In other words... $r^2$ really is the proportion of the variance in y (the $\textrm{OBSERVED}$ thta comes from the model $\textrm{VAR(FIT)}$)

We all recall that $r^2$ is a number that ranges from 0 to 1 and is **also** a good way of thinking about how close the data is to being on a straight line.

Also notice that under this defintiion it really does not matter how many explanatory variables are in the model.

R uses an **adjusted** $r^2$ to penalize a model with more parameters.

This adjusted-$r^2$ is one measure of whether or not a linear model is a good fit for the data.

We could also write the equation for $r^2$ as

$$
\begin{aligned}
r^2 &= \frac{\sum(y_i-\widehat{y_i})^2}{\sum(y_i-\overline{y})^2}\\
&=\frac{\textrm{Sum of Squares of Residuals}}{\textrm{Sum of Squares of Observations}}
\end{aligned}
$$

## PRESS Statistic
The R package `qpcr` has a useful variant of MSE called **Allen's PRESS** (Predicted Sum of Squares) and a varient of $r^2$ called  **p square**.  The idea for PRESS is to leave out each row in the table, use the remaining data to generate a model, then predict the response value for the omitted row.  The closer this value is to 0 ,the more internally consistent the data is with the model being employed.

The PRESS statistic, clearly depends rather strongly on the number of observations... we can standardize this value in a fashion that is similar to what we did for $r^2$.

Let $\widehat{y_i}$ be the predicted value for the $i$th observation and $y_i$ the true value.  If we let $\epsilon_i = y_i - \widehat{y_i}$ then, in analogy to $r^2$:

$$
p^2 = \frac{\sum\epsilon_i^2}{\sum(y_i-\overline{y})^2}
$$

https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/

The `PRESS()` function that will calculate p-square does not seem to deal well with `NA`s so we'll want to remove them:

```{r,message=FALSE,warning=FALSE,echo=TRUE}
library(qpcR)
reduced.air.quality=airquality[!is.na(airquality$Ozone),]
linear.model=lm(data=reduced.air.quality,Ozone~Temp)
summary(linear.model)
press=PRESS(linear.model) #Calculated the p square statistic
plot(press$residuals)
press$P.square #The press statistic that we care about.
```

The closer to 0 the better this statistic.  Once the `NA`'s are removed the `qpcR` library will allow the PRESS and $p^2$ statistic to be produced for linear models as well as generalized linear models and even regression trees (all of which we will discusss later)

[Here](https://www.analyticbridge.datasciencecentral.com/profiles/blogs/use-press-not-r-squared-to-judge-predictive-power-of-regression) is an interesting, short, article on using the PRESS statistic as a measure of predictive power.  In particular, the example uses several models with similar $r^2$ values but rather different $p^2$ values... 

## Logistic Regression

With Generalized Linear Models (see the Mathematical Background for more details) we can attempt to make predictions about point-clouds that do not have a nice linear relationship.

One common situation arises when there is some sort of a yes/no question (the response) where the probability of "success" (getting a "yes") can change based upon the explanatory variables.  

In the simplest situation we have one continous explanatory variable, $x$, and one response variable $y$.  We use the following coding:

Result         |  Code
---------------|--------
$\textrm{no}$  | 0
$\textrm{yes}$ | 1

For any given observation $i$ there is a probability (based upon $x_i$) of $\pi_i$ for getting a 1 and $1-\pi_1$ of getting a 0.  All the observations with the same value of $x$ should have the **same** probability of getting a 0 (or a 1).  If we group all the observations with the same values for $x$ we can think of the data as having $m$ bins.  The distribution of $y$ in each bin will be a Binomial distribution.

Review the Logistic Regression section of MathBackground for more information on the details.  For now we will focus on being very practical.  


(heavily borrowing from https://towardsdatascience.com/simply-explained-logistic-regression-with-example-in-r-b919acb1d6b3)
I'd like to predict whether or not an applicant to UCLA is going to be admitted.  It seems reasonable that their high school GPA, their GRE score, and the rank (a proxy for quality) of their high school will influence the probability of them being admitted.  

Let's pull the data from UCLA:

```{r,echo=TRUE}
df <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
```

Do some basic summarization and graphical exploration of the data before proceeding.  I expect to see

* numeric summaries
* graphical summaries (single variables)
* graphical summaries (like side-by-side boxplot) comparing multiple variables

```{r ucla-example, exercise=TRUE, exercise.lines=10}
df <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")

```

Let me remind you that the goal of regression, as we have been practicing it, is to find the **middle** of the $y$-values that would result... in other words our model should be predicting the *probability* (for the given combination of explanatory values).  If all our model assumptions are satisfied the curve of best fit for this prediction is a logistic curve.

Our data has three explanatory variables and yes/no response.

We use the `glm()` function and must specify the keyword argument `family="binomial"'.  The resulting parameters tell us something about how the explanatory variables modify the "log odds".

Let's actually perform the logistic regression.  We will start by only using `gre` and `gpa` because 

```{r,echo=TRUE}
model=glm(data=df,family="binomial",admit~gre+gpa)
summary(model)
```

What are we seeing?

The equation is predicting the log odds:

$$
\log\left(\frac{p}{1-p}\right) = -4.93 + 0.003\ \textrm{gre} + 0.755\ \textrm{gpa}
$$

Note that $p=0$ or $p=1$ result in undefined expressions since $\log(0)$ is undefined and so is $\frac{1}{0}$.  

Exponentiating both sides produces an equation modeling the odds:

$$
\begin{aligned}
\frac{p}{1-p} &= e^{-4.93 + 0.003\ \textrm{gre} + 0.755\ \textrm{gpa}}\\
&= e^{-4.93}e^{0.003\ \textrm{gre}}e^{0.755\ \textrm{gpa}}\\
&=(0.007) 1.003^{\textrm{gre}}2.127^{\textrm{gpa}}
\end{aligned}
$$
Increasing $\textrm{gre}$ by 1 will multiply the RHS by 1.003.  In other words it increases the odds by a factor of 1.003.  The effect size might not be high... but a GRE score can vary quite a bit... increasing it by 100 increases the odds by $1.003^{100}=1.349$.  GPA on the other hand, which has a much more limited range, has a larger effect on a **per-point** basis (whether or not such a distinction is important I will leave for you to consider).

Notice that $1+odds = \frac{1}{1-p}$
$$
\begin{aligned}
\frac{odds}{1+odds}&= \frac{p}{1-p}\frac{1}{1+\frac{p}{1-p}}\\
&=\frac{p}{1-p}\frac{1}{\frac{1-p}{1-p}+\frac{p}{1-p}}\\
&=\frac{p}{1-p}\frac{1}{\frac{1}{1-p}}\\
&=\frac{p}{1-p}\frac{1-p}{1}\\
&=p
\end{aligned}
$$

So although we can turn information about the odds into information about probability... a straightforward interpretation is still a bit problematics.

Let's train our intuition a bit by looking at just `gpa`:

```{r}
plot(df$gpa,jitter(df$admit,0.1),col=rgb(0,0,0,0.4),pch=".",cex=3)
```

The y-axis is quite boring-- just 0/1.  But by using some jitter and transparency to deal with overplotting we see that an increase in GPA really does seem to improve the probability of being accepted.

The model of this is

```{r,echo=TRUE}
onlygpa=glm(data=df,admit~gpa,family="binomial")
summary(onlygpa)
```

So

$$
\begin{aligned}
odds&=(0.0128)2.8608^{\textrm{gpa}}\\
\frac{odds}{1+odds} &= \frac{(0.0128)2.8608^{\textrm{gpa}}}{1+(0.0128)2.8608^{\textrm{gpa}}}\\
p&=\frac{1}{(78.0686)2.8608^{-\textrm{gpa}} +1}
\end{aligned}
$$
```{r}
plot(df$gpa,jitter(df$admit,0.1),col=rgb(0,0,0,0.4),pch=".",cex=3,ylab="")
.f=function(x){
  1/(1+78.0686*2.8608^(-x))
}
curve(.f(x),-2,4,add=TRUE)
```

That's not too shabby.  If you have a 4.0 GPA your probability of acceptance is about 46% and that's what the model predicts.

```{r}
table(df$admit[df$gpa==4])
13/28
.f(4)
```

**Exercise:** Repeat the process above but use $\textrm{gre}$ as the explanatory variable.  Produce the model and graph the logistic curve for $p$ with explanatory variable $\textrm{gre}$.  Check the prediction for $\textrm{gre}=800$ against the actual sample probability.

```{r gre,exercise=TRUE, exercise.lines=10}

```

NOTE: Up above I solved the equation for $p$ algebraically but sometimes it is easier to just let the computer do it for us.  The expression `fitted.value(model)` will generate the fitted values.  I will redo the logistic regression using $\textrm{gpa}$ and $\textrm{gre}$, then produce a formula that will convert the explanatory values to the fitted values.  We can then compare the output of `fitted.values()` to our results.  Notice that we have **two** explanatory variables... so we'll need to be a bit fancier in our graphing.  I will use the graphing package `plotly`

```{r,echo=TRUE,message=FALSE,warning=FALSE}
library(plotly)
model = glm(data=df,admit~gpa+gre,family="binomial")
fv=fitted.values(model)
plot_ly(data=df,size=0.2) %>% add_markers(x=~gpa,y=~gre,z=~fv) %>% layout(scene = list(xaxis = list(title = 'GPA'), 
                    yaxis = list(title = 'GRE'),
                    zaxis = list(title = 'Probability'))) #%>% add_segments()

```
Now let's see if we correctly understand the equation.  First we get the coefficients from the model:

```{r,echo=TRUE}
coefficients(model)
```

These three values fit into the equation:

$$
\log\left(\frac{p}{1-p}\right)=-4.944 + 0.755\textrm{ gpa}+0.003\textrm{ gre}
$$

The left hand side is the *log odds* (aka logit).  Let's capture this in an equation.  Make sure you understand how my expression matches the RHS of the equation.

```{r,echo=TRUE}
log.odds=function(x){ #Assume variable model is defined
  sum(coefficients(model)*c(1,as.numeric(x[c("gpa","gre")])))
}
```

Now let's turn this into the actual probabilities:

```{r,echo=TRUE}
logistic=function(lo){
  1/(exp(-1*log.odds(lo))+1)
}
```

We can use `apply` to apply this function to every row in the data.frame to compare our values to the fitted values:

```{r,echo=TRUE}
our.values=apply(df,1,logistic)
summary(our.values-fv)
```

Notice how close our values are... just a few rounding issues (`-1.110e-16` and `1.110e-16` are VERY close to 0).

So... success... we understand how to do this.

Now **you** do the same thing, but with the full model.  Be sure to put your summary into a variable called 
`my.summary`

```{r checking-logistic-equation,exercise=TRUE, exercise.lines=10}
```

<!--
http://www.statisticalassociates.com/logistic10.htm

http://www.statisticalassociates.com/booklist.htm

{r simulation-exercise exercise=TRUE,exercise.lines=10}

-->
Look at the first few lines of the table:

```{r,results="asis"}
rf=head(df,5)
rf%>%kable()%>%kable_styling()%>%row_spec(which(rf$admit==1), background = "#CAEFCA",bold=T)
```

I have highlighted the rows that were associated to admitted individuals.  

Let's build a full model (using `rank` as a factor()) and predict "admit" and make some predictions about admittance using our model.  The idea is that we let the explanatory variables predict the probability.  

Some people use logistic regression for classification. [There are good arguments against this](https://www.fharrell.com/post/classification/) (you should follow and read that link).  

<!-- add quiz about this later -->
But, that said, we could try it anyway... we need to make some decision... will we call a row isn the table a "yes" or a "no".  One approach would be to say that anything *at or above* 0.5 counts as a yes(1).  And anything below counts as a no (0).

## Confusion Matrices

Let's build the full model and make our predictions:
```{r,echo=TRUE}
full.model=glm(data=df,admit~gpa+gre+factor(rank),family="binomial")
fv=fitted.values(full.model)
predicted.cat=ifelse(fv>=0.5,1,0)
table(df$admit,predicted.cat)
```

The rows are the actual values, and the cols are the predicted values.  This table is known as the **confusion matrix** (because the off-diagonals measure the mistakes)

You can see that for 254 cases our algorithm predicted "no" and that was the result.  In 30 cases, our algorithm predicted "yes" and that was the result.  In 19 cases our algorithm predicted "yes" when the true situations was "no".  And the most extreme situation mistakes occurred when we predicted "no", but the true situation wasy "yes"

Each of the squares in the table have names:

* **True Positives**:  The observed and predicted value were both 1 (30 of these for us)
* **True Negatives**:  The observed and predicted value were both 0 (254 of these for us)
* **False Positives**: Prediction was "yes", but observed was "no" (19 of these for us)
* **False Negatives**: Prediction was "no", but observed was "yes" (97 of these for us)

False Positives are also known as **type I errors** and False Negatives are also known as **type II errors**.  (Think about that for a bit in the context of what you learned in Introduction to Statistics and Hypothesis Tests)


Make the model in the console use $\textrm{gpa}$ and $\textrm{gre}$ as the predictors.  Use 0.5 as the prediction threshold.  Answer the following questions

```{r ML-quiz,echo=FALSE}
quiz(
  question("The number of true positives",
    answer("263"),
    answer("10"),
    answer("118"),
    answer("9", correct = TRUE)
  ),
    question("The number of false positives",
    answer("263"),
    answer("10", correct = TRUE),
    answer("118"),
    answer("9")
  ),
    question("The number of true negatives",
    answer("263", correct = TRUE),
    answer("10"),
    answer("118"),
    answer("9")
  ),
    question("The number of false negatives",
    answer("263"),
    answer("10"),
    answer("118", correct = TRUE),
    answer("9")
  )
)
```

It's not too surprising that the number of false negatives is the highest category.  If you **ignored every predictor** your best guess would be "no".  And this simple algorithm woudl give you the proper predictor 68.25% of the time!  

Let's call this the **accuracy**.  In other words the proportion of predictions that were correct.  

$$
\textrm{Accuracy} = \frac{\textrm{True Positives}+\textrm{True Negatives}}{\textrm{Total}}
$$

So... in our example the "always guess *no*" approach has an accuracy of over 68%.  Notice that the logistic regression approach from your last exercise (using a threshold of 0.5) has an accuracy of $\frac{263+9}{400} = 68%$.  Hmmm... that wasn't very satisfying was it?  Even including the `rank` (as a factor) only changes the accuracy to 71%

There does seem to be something fundamentally different about the two strategies... The "always say no" approach will never yield a false positive (nor a true positive).  The logistic regression prediction approach is a bit more balanced.  

Let's see what effect changing our threshold has.  Let's go back to the full model and make an accuracy curve:

```{r,echo=TRUE}
full.model=glm(data=df,admit~gpa+gre+factor(rank),family="binomial")

rc=function(threshold,m){
  tmp=ifelse(fitted.values(m)>=threshold,1,0)
  if(all(tmp==0)){return(sum(df$admit==0)/nrow(df))}
  if(all(tmp==1)){return(sum(df$admit==1)/nrow(df))}
  tt=table(tmp,df$admit)
  (tt[1,1]+tt[2,2])/sum(tt)
}
rcv=Vectorize(rc,vectorize.args="threshold")
results=curve(rcv(x,full.model),0,1)
results$x[which.max(results$y)]
results$y[which.max(results$y)]

```

We can get up to 71.5% if we use 49% as a threshold... lets' look a bit closer:

```{r}
results=curve(rcv(x,full.model),0.45,0.55)
results$x[which.max(results$y)]
results$y[which.max(results$y)]
```

A very minor improvement by changing the threshold to 48.6%... still that's pretty strange.

Depending upon your purposes you might be willing to have fewer accurate predictions as long as you have fewer false positives... take, for example, a spam filter (which we'll discuss below in the context of Naive Bayes and the bag of words model).

It is far better for some real spam slip into a persons inbox then for a real communication to be classified as spam and filtered.  In that situation, we might use logistic regression and set a threshold of 0.9.

in that situation we would be far more concerned with the **precision**-- that's the percentage of things classified as "yes" that are correct:

$$
\textrm{Precision} = \frac{\textrm{True Positives}}{\textrm{True Positives}+\textrm{False Positives}}
$$
Another useful measure is **recall** (also known as **sensitivity** also known as **true positive rate**)- this measures the percentage of the **actual positives** that  were labeled correctly

$$
\textrm{Recall} = \frac{\textrm{True Positives}}{\textrm{True Positives}+\textrm{False Negatives}}
$$

Read that a couple of times (it always confuses me the first time I review the material).  THe true positives are the cases labeled positive that **are** positive.  The False negatives are the mislabeled **actual positives**.  So the recall (or sensitivity) is a measure of how many of the positives are identified (ignoring false positives).

It's easy to get a high recall-- just adopt the "always say yes" strategy.  Then EVERYTHING is a positive... and so the recall is always 100%.  

We also have **specificity** (also known as the **true negative rate**)

$$
\textrm{Specificity} = \frac{\textrm{True Negatives}}{\textrm{True Negatives}+\textrm{False Positives}}
$$

Let's think of the possible results that could occur for test that tries to detect a disease:

metric         |    interpretation
---------------|--------------------
Accuracy       | P(test result is correct)
Sensitivity    | P(result is positive | patient has disease)
Specificity    | P(result is negative | patient does not have disease)

There's one other common measure you might encounter... It's called $F1$ and it tries to balance the true positive rate and the true negative rate.

$$
\textrm{F1} = 2\left(\frac{\textrm{Precision}*\textrm{Recall}}{\textrm{Precision}+\textrm{Recall}}\right)
$$
Notice that if our accuracy was **perfect** then $\textrm{Precision}$ and $\textrm{Recall}$ would both be 1.  In this situation $F1$ would be 1.  If either $\textrm{Precision}$ and $\textrm{Recall}$ is 0, then the numerator is 0 and the denominator is **not 0** (they can't both be 0 at the same time).  This is actually (after some algebraic manipualtion) the **harmonic mean** of $\textrm{precision}$ and $\textrm{recall}$.

<!-- cut out the decision tree and later stuff... make that into regression 2 -->