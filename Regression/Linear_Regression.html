<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<meta name="progressive" content="false" />
<meta name="allow-skip" content="false" />

<title>Regression Tutorial</title>


<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<p>This tutorial is designed to provide some basic (hopefully review) background material on linear regression along with some practical examples.</p>
<p>Let’s start with a review of the <strong>idea</strong> behind linear regression. It is assumed that most of this is review and a number of terms will <strong>not be formally defined</strong></p>
<div id="section-regression" class="section level2">
<h2>Regression</h2>
<p>Let’s start with some interesting data with variables that might exhibit a reasonably linear relationships. Using <a href="http://rpubs.com/Nitika/linearRegression_Airquality">Nitika’s rpub</a> as inspiration, our first example will utilize the built-in <code>airquality</code> dataset (see <code>help(topic=&quot;airquality&quot;)</code> for more details). Let’s first look at the relationship between the various variables using the <code>pairs()</code> function:</p>
<p><img src="Linear_Regression_files/figure-html/unnamed-chunk-1-1.png" width="624" /></p>
<p>When <code>Temp</code> is used as the <strong>explanatory</strong> variable and <code>Ozone</code> as the <strong>response</strong> variable the resulting scatterplot shows a positive association:</p>
<p><img src="Linear_Regression_files/figure-html/unnamed-chunk-2-1.png" width="624" /></p>
<p>Notice our use of <code>~</code> in the <code>plot()</code> function up above. In <strong>simple linear regression</strong> R uses the function <code>lm()</code> and model notation which has the response variable on the left and the explanatory on the right: <code>response~explanatory</code>. Since the response variable is traditionally represented as the <em>y</em>-variable and the explanatory as the <em>x</em>-variable this can cause a bit of confusion when used in <code>plot()</code>.</p>
<div id="section-linear-models" class="section level3">
<h3>Linear models</h3>
<p>We can use any line as a model for the relationship between <code>Temp</code> and <code>Ozone</code>, but some lines are better than others. In the interactive graph below the “residual bars” represent the distance from each data-point to the proposed line. The sum of the <strong>squares</strong> of these lengths is called the <strong>sum of squares</strong> for the line and represents a measure of <strong>goodness of fit</strong>:</p>
<p><div class="form-group shiny-input-container">
<label class="control-label" for="slope">Slope:</label>
<input class="js-range-slider" id="slope" data-min="-4" data-max="4" data-from="3" data-step="0.25" data-grid="true" data-grid-num="8" data-grid-snap="false" data-prettify-separator="," data-prettify-enabled="true" data-keyboard="true" data-data-type="number"/>
</div><div class="form-group shiny-input-container">
<label class="control-label" for="intercept">Intercept:</label>
<input class="js-range-slider" id="intercept" data-min="-200" data-max="-100" data-from="-160" data-step="1" data-grid="true" data-grid-num="10" data-grid-snap="false" data-prettify-separator="," data-prettify-enabled="true" data-keyboard="true" data-data-type="number"/>
</div><div class="form-group shiny-input-container">
<div class="checkbox">
<label>
<input id="showResiduals" type="checkbox"/>
<span>Show Residual Bars</span>
</label>
</div>
</div><div class="form-group shiny-input-container">
<div class="checkbox">
<label>
<input id="showSS" type="checkbox"/>
<span>Show Sum of Squares</span>
</label>
</div>
</div><div id="lmPlot" class="shiny-plot-output" style="width: 100% ; height: 400px"></div></p>
<p>The <strong>line of regression</strong> or <strong>line of best fit</strong> is the <em>unique</em> line (we don’t prove uniqueness in this class… but it is) that minimizes the sum of squares.</p>
<p>We can view this line as a <strong>model</strong> with two <strong>parameters</strong>:</p>
<ul>
<li>the <strong>intercept</strong>, and the</li>
<li>the <strong>slope</strong>.</li>
</ul>
<p>The values that we calculate from the data are an <strong>approximation</strong> to the <strong>true</strong> values that express the actual linear relationship (as much as there is one) between the Temperature in New York in 1973 and the Ozone levels.</p>
<p>We can use the linear model to:</p>
<ul>
<li><strong>Describe</strong>, or to</li>
<li><strong>Predict</strong>.</li>
</ul>
<p>If the prediction occurs within the range of previously observed values (in this case between around 50 degrees F and 100 degrees F) then this is a form of <strong>interpolation</strong>. If the prediction occurs outside the range then we are <strong>extrapolating</strong>.</p>
<p>In classic R we use the function <code>lm()</code> to produce the model. Frequently we store the results in a variable and use the R command <code>summary()</code> for numeric information and <code>abline()</code> to add the line to the model:</p>
<pre class="r"><code>linear.model&lt;-lm(data=airquality,Ozone~Temp)
summary(linear.model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Temp, data = airquality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***
## Temp           2.4287     0.2331  10.418  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
##   (37 observations deleted due to missingness)
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>with(airquality,{
  plot(Ozone~Temp,
       xlab=&quot;Temp (degrees F)&quot;,
       ylab=&quot;Ozone (ppb)&quot;,
       main=&quot;New York air quality May to September of 1973&quot;)
  abline(linear.model)
})</code></pre>
<p><img src="Linear_Regression_files/figure-html/unnamed-chunk-5-1.png" width="624" /></p>
</div>
<div id="section-linear-regression-exercise" class="section level3">
<h3>Linear Regression Exercise</h3>
<p>In the code exercise below perform the following actions:</p>
<ul>
<li>Determine how many <code>NA</code>’s exist in the <code>Ozone</code> variable of <code>airquality</code> and store in a variable called <code>n.NAs</code></li>
<li>Using the <code>lm()</code> function in R, find the line of best fit for the relationship between <code>Wind</code> and <code>Ozone</code>. Store the model in a variable called <code>my.model</code>.</li>
<li>Use <code>abline()</code> to add the line of best fit to the graph</li>
</ul>
<div class="tutorial-exercise" data-label="two-plus-two" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="6">
<pre class="text"><code>n.NAs&lt;-#Your code here
with(airquality,{
  plot(Ozone~Wind,xlab=&quot;Wind Speed (mph)&quot;,ylab=&quot;Ozone (ppb)&quot;,main=&quot;New York air quality May to September of 1973&quot;)
  abline(&quot;stuff here&quot;)
})</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"default","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.checker":"NULL"}</script>
</div>
</div>
</div>
<div id="section-testing-the-model" class="section level2">
<h2>Testing the model</h2>
<p>For any pair of variables it is always possible to make a line of best fit. The utility of that line depends upon the actual relationship between the two variables. For example data that follows a nice <strong>quadratic</strong> relationship (think <em>parabola</em>) will still have a line of best fit but it might not be particularly useful:</p>
<p><img src="Linear_Regression_files/figure-html/unnamed-chunk-6-1.png" width="624" /></p>
<p>One way to determine the appropriateness of the model is to look at the <strong>residual plot</strong> which plots the <strong>residuals</strong> against the <strong>fitted values</strong>. Recall that for each data point the <strong>observed value</strong> is the data itself, the <strong>fitted value</strong> (also known as the <strong>predicted value</strong>) is value predicted for the response variable by plugging in the value of the explanatory variable into the model, and the <strong>residual</strong> is the difference between the observed value (the data) and the predicted (or fitted value)).</p>
<p>In a good model there is no relationship between the value of the residual and the fitted value. This is known as <strong>homoscedasticity</strong>. In a bad model the residual exhibit <strong>heteroscedasticity</strong>.</p>
<p>R will generate the residual plot as one of the diagnostic plots associated to a linear model, but we will use the following <strong>convenience functions</strong> to make it ourselves. As you read the code below remember that <code>my.model</code> was generated in the previous code chunk:</p>
<p><img src="Linear_Regression_files/figure-html/unnamed-chunk-7-1.png" width="624" /></p>
<p>We see that there is quite a strong relationship beteen the residual and the fitted value.</p>
<p>Let’s look at another common problem. Below, you will clearly see that the <strong>point cloud</strong> varies in “height” for different values of <span class="math inline">\(x\)</span>. This means that there is an association between the explanatory value (of <span class="math inline">\(x\)</span>) and the <strong>variance</strong>.</p>
<p><img src="Linear_Regression_files/figure-html/unnamed-chunk-8-1.png" width="624" /><img src="Linear_Regression_files/figure-html/unnamed-chunk-8-2.png" width="624" /></p>
<p>Recall the ideas of <strong>probability</strong>, <strong>distributions</strong>, and <strong>conditional probability</strong>. The key concepts that we care about are <strong>random</strong>, <strong>expected value</strong> (a measure of the <strong>center</strong> of a distribution), and <strong>variance</strong> ( a measure of the <strong>spread</strong> of a distribution).</p>
<p>The graph of a <strong>function</strong> must pass the <strong>vertical line test</strong>– which means, graphically, that for any given input only one output is possible. These point clouds frequently do not satisfy those requirements… instead for any given value of <span class="math inline">\(x\)</span> (the explanatory variable) there may be <strong>multiple</strong> values in <span class="math inline">\(y\)</span> (the response variable). For any <strong>given</strong> value of <span class="math inline">\(x\)</span> there is a <strong>distribution</strong> of values in <span class="math inline">\(y\)</span>. When the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is appropriate (which I’m not going to define carefully) the line of best fit is <strong>really</strong> expressing the <strong>expected value</strong> of the distribution of <span class="math inline">\(y\)</span> for a given value of <span class="math inline">\(x\)</span>. In other words:</p>
<p><span class="math display">\[
\mathbb{E}\textrm{(y|x)}
\]</span></p>
<p>The line-of-best-fit technology (linear regression) can always be applied and used to make predictions, but the standard techniques for assessing the liklihood of those predictions relies upon <strong>constant variance</strong> (what aka… <strong>homoscedasticity</strong>). If those conditions are satisfied then confidence intervals for</p>
<ul>
<li><strong>slope</strong></li>
<li><strong>intercept</strong></li>
<li><strong><span class="math inline">\(\mathbb{E}\textrm{(y|x)}\)</span></strong></li>
<li><strong><span class="math inline">\(\textrm{value of y given x}\)</span></strong></li>
</ul>
<p>are all theoretically accurate. This occurs when the following relationship holds:</p>
<p><span class="math display">\[
\textrm{DATA = FIT + RESIDUAL}
\]</span> Where <span class="math inline">\(\textrm{DATA}\)</span> is the response variable. <span class="math inline">\(\textrm{FIT}\)</span> is the <strong>fitted value</strong> predicted by the linera model and <span class="math inline">\(\textrm{RESIDUAL}\)</span> is the <strong>residual</strong>. Furthermore, we need</p>
<p><span class="math display">\[
\textrm{RESIDUAL} \sim N(0,\sigma^2)
\]</span></p>
<p>In other words the residual (also known as the <strong>error</strong>) is normally distributed with a mean of 0 and variance of <span class="math inline">\(\sigma^2\)</span>.</p>
<p><strong>When</strong> those requirements are not satisifed then the values provided by functions like <code>summary()</code> may not be valid.</p>
<p>For a dense enough point cloud, One can pick a specific <span class="math inline">\(x\)</span> value (or very small range of <span class="math inline">\(x\)</span>-values) and observe the corresponding <span class="math inline">\(y\)</span> distribution (these values are an estimate for the distribution of <span class="math inline">\(y\)</span> <strong>given</strong> <span class="math inline">\(x\)</span> (we use the notation <span class="math inline">\(p(y|x)\)</span>). The <strong>expected value</strong> (or <strong>mean</strong>) of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> is not very useful if the distribution is skewed.</p>
<p>Packages such as <code>ggplot2</code> contain code to generate</p>
<p>geom_smooth() confint()</p>
<p>Even if the conditional distributions of <span class="math inline">\(y\)</span> are all relatively symmetric, the spread could vary from <span class="math inline">\(x\)</span> value to <span class="math inline">\(x\)</span> value– resulting in heteroscedasticity. Various techniques may be employed to transform the original data into a form that is closer to the requirements for a good linear model. One of these <strong>variance stabilizing</strong> transformations is to take the <span class="math inline">\(\log(y)}\)</span> of a variables values, or <span class="math inline">\(\sqrt{y}\)</span></p>
<p>This is useful, but complicates interpretation.</p>
<div id="section-explanatory-variables" class="section level3">
<h3>Explanatory Variables</h3>
<p>The <strong>linear</strong> in linear regression refers to the relationship between explanatory variables. The variables, themselves, can be the result of non-linear processes. A simple example would be to fit a quadratic curve. <strong>Dummy Variables</strong> are another good example</p>
<p>EXPAND ### Measuring model fit</p>
<p>We have already seen that the line of best fit in Ordinarly Least Squares regression is the one that minimizes the Sum of Squares of the error. Dividing this value by the number of observarionts <span class="math inline">\(n\)</span> for the model results in the MSE (Mean Sum of squares of error), which is, essentially, the variance of the error terms. Also note that minimizes SSE also minimizes MSE (and vice versa). It is related to the <span class="math inline">\(r^2\)</span> statistic.</p>
<p><span class="math display">\[
r^2 = 1 - \frac{\sum (y_i-\widehat{y_i})}{\sum{(y_i-\overline{y})}}
\]</span> Recall that if the errors are independent of the model:</p>
<p><span class="math display">\[
\begin{aligned}
\textrm{OBSERVED} &amp;= \textrm{FIT} + \textrm{ERROR}\\
\textrm{VAR(OBSERVED)} &amp;= \textrm{VAR(FIT)}+ \textrm{VAR(ERROR)}
\end{aligned}
\]</span></p>
<p>Notice further that:</p>
<p><span class="math display">\[
\textrm{VAR(FIT)} = \textrm{VAR(OBSERVED)}- \textrm{VAR(ERROR)}
\]</span></p>
<p>So that expression for <span class="math inline">\(r^2\)</span> when algebraically manipulated into the form of a single fraction ends up being:</p>
<p>$$</p>
<p>r^2 =  $$ In other words… <span class="math inline">\(r^2\)</span> really is the proportion of the variance in y (the <span class="math inline">\(\textrm{OBSERVED}\)</span> thta comes from the model <span class="math inline">\(\textrm{VAR(FIT)}\)</span>)</p>
<p>We all recall that <span class="math inline">\(r^2\)</span> is a number that ranges from 0 to 1 and is <strong>also</strong> a good way of thinking about how close the data is to being on a straight line.</p>
<p>Also notice that under this defintiion it really does not matter how many explanatory variables are in the model.</p>
<p>R uses an <strong>adjusted</strong> <span class="math inline">\(r^2\)</span> to penalize a model with more parameters.</p>
<p>This adjusted-<span class="math inline">\(r^2\)</span> is one measure of whether or not a linear model is a good fit for the data.</p>
<p>The R package <code>qpcr</code> has a useful variant of <span class="math inline">\(r^2\)</span> called <strong>Allen’s PRESS</strong> (Predicted Sum of Squares) (aka <strong>p square</strong>). The idea is to generate <a href="https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/" class="uri">https://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/</a></p>
<p>Recall from earlier our linear model of air quality. The <code>PRESS()</code> function that will calculate p-square does not seem to deal well with <code>NA</code>s so we’ll want to remov ethem:</p>
<pre><code>## Loading required package: MASS</code></pre>
<pre><code>## Loading required package: minpack.lm</code></pre>
<pre><code>## Loading required package: rgl</code></pre>
<pre><code>## Warning in rgl.init(initValue, onlyNULL): RGL: unable to open X11 display</code></pre>
<pre><code>## Warning: &#39;rgl_init&#39; failed, running with rgl.useNULL = TRUE</code></pre>
<pre><code>## Loading required package: robustbase</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## Call:
## lm(formula = Ozone ~ Temp, data = reduced.air.quality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.729 -17.409  -0.587  11.306 118.271 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***
## Temp           2.4287     0.2331  10.418  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 23.71 on 114 degrees of freedom
## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 
## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre><code>## .........10.........20.........30.........40.........50
## .........60.........70.........80.........90.........100
## .........110......</code></pre>
<pre><code>## $stat
## [1] 65944.17
## 
## $residuals
##   [1]  25.7892245   8.2271809 -20.9393851  14.9072856  15.0354518
##   [6]  12.4360833  23.7229498   7.1011396 -25.9902405  -4.6606594
##  [11]  -2.3512167  -4.2326783  25.3149514   5.7139853  21.1719230
##  [16]  15.3372823  12.0612136   7.6686741   4.9134541 -19.5128028
##  [21]   2.9512108  32.0007123   7.4210996  -4.7751612  70.7465338
##  [26]  -0.5912572 -23.3984497  -0.6015453 -25.7307718 -29.4607007
##  [31] -19.1814260   9.2392770   9.3603258 -18.5017689 -24.8082145
##  [36]  78.9516918 -10.5875706 -17.9007349   9.5193856 -14.7515613
##  [41]  10.4635614  21.1450469  21.1450469  16.1739407 -20.5238367
##  [46] -22.9490325 -40.6691907  -1.7461826 -17.3361988   4.0338340
##  [51]  14.9475625   3.6044893 -16.8987009  18.4031944  49.2218247
##  [56] -32.4918261 -10.0234207  15.5579961 -12.0538932   9.5193856
##  [61]   9.3600720 -10.8331183 -41.1229037 -36.5333268  16.3727219
##  [66] -24.7796305   1.7271039  53.9518402  17.8191698  39.3100040
##  [71] -18.1453107 -24.4088249  17.8610275 -18.1726558  14.2525534
##  [76] -14.7178156 -11.5428932   1.5701717 -19.1814260 -19.0994153
##  [81]   0.1290584 119.4129592  11.2965407 -13.1679390  37.9797100
##  [86]  -2.2509312   3.8265792  22.5536551   1.5994348  -6.0602230
##  [91]  12.5106940 -17.5951049 -25.3258671 -27.5503487 -19.6124715
##  [96] -14.2918451  -6.3693622  -5.7848207 -16.7358954 -12.1200346
## [101] -16.6620999 -12.6086635   3.5875663   2.3188426 -24.8082145
## [106]   5.9510041 -36.5333268   4.6860090  -2.4750725 -13.8620968
## [111] -13.8090989   8.2341470   7.0898255 -21.3583917 -19.7630151
## [116]   1.8775312
## 
## $P.square
## [1] 0.4730497</code></pre>
</div>
</div>
<div id="section-logistic-regression" class="section level2">
<h2>Logistic Regression</h2>
<p>With Generalized Linear Models (see the Mathematical Background for more details) we can attempt to make predictions about point-clouds that do not have a nice linear relationship.</p>
<p>One common situation arises when there is some sort of a yes/no question (the response) where the probability of “success” (getting a “yes”) can change based upon the explanatory variables.</p>
<p>In the simplest situation we have one continous explanatory variable, <span class="math inline">\(x\)</span>, and one response variable <span class="math inline">\(y\)</span>. We use the following coding:</p>
<table>
<thead>
<tr class="header">
<th>Result</th>
<th>Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\textrm{no}\)</span></td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\textrm{yes}\)</span></td>
<td>1</td>
</tr>
</tbody>
</table>
<p>For any given observation <span class="math inline">\(i\)</span> there is a probability (based upon <span class="math inline">\(x_i\)</span>) of <span class="math inline">\(\pi_i\)</span> for getting a 1 and <span class="math inline">\(1-\pi_1\)</span> of getting a 0. All the observations with the same value of <span class="math inline">\(x\)</span> should have the <strong>same</strong> probability of getting a 0 (or a 1). If we group all the observations with the same values for <span class="math inline">\(x\)</span> we can think of the data as having <span class="math inline">\(m\)</span> bins. The distribution of <span class="math inline">\(y\)</span> in each bin will be a Binomial distribution.</p>
<p>Review the Logistic Regression section of MathBackground for more information on the details. For now we will focus on being very practical.</p>
<p>(heavily borrowing from <a href="https://towardsdatascience.com/simply-explained-logistic-regression-with-example-in-r-b919acb1d6b3" class="uri">https://towardsdatascience.com/simply-explained-logistic-regression-with-example-in-r-b919acb1d6b3</a>) I’d like to predict whether or not an applicant to UCLA is going to be admitted. It seems reasonable that their high school GPA, their GRE score, and the rank (a proxy for quality) of their high school will influence the probability of them being admitted.</p>
<p>Let’s pull the data from UCLA:</p>
<p>Do some basic summarization and graphical exploration of the data before proceeding. I expect to see</p>
<ul>
<li>numeric summaries</li>
<li>graphical summaries (single variables)</li>
<li>graphical summaries (like side-by-side boxplot) comparing multiple variables</li>
</ul>
<p>Let me remind you that the goal of regression, as we have been practicing it, is to find the <strong>middle</strong> of the <span class="math inline">\(y\)</span>-values that would result… in other words our model should be predicting the <em>probablility</em>. If all our model assumptions are satisfied the curve of best fit for this prediction is a logistic curve.</p>
<p>Our data has three explanatory variables and yes/no response.</p>
<p>We use the <code>glm()</code> function and must specify the keyword argument `family=“binomial”’. The resulting parameters tell us something about how the explanatory variables modify the “log odds”.</p>
<p>Let’s actually perform the logistic regression. We will start by only using <code>gre</code> and <code>gpa</code> because</p>
<pre><code>## 
## Call:
## glm(formula = admit ~ gre + gpa, family = &quot;binomial&quot;, data = df)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.2730  -0.8988  -0.7206   1.3013   2.0620  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -4.949378   1.075093  -4.604 4.15e-06 ***
## gre          0.002691   0.001057   2.544   0.0109 *  
## gpa          0.754687   0.319586   2.361   0.0182 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 480.34  on 397  degrees of freedom
## AIC: 486.34
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>What are we seeing?</p>
<p>The equation is predicting the log odds:</p>
<p><span class="math display">\[
\log\left(\frac{p}{1-p}\right) = -4.93 + 0.003\ \textrm{gre} + 0.755\ \textrm{gpa}
\]</span></p>
<p>Note that <span class="math inline">\(p=0\)</span> or <span class="math inline">\(p=1\)</span> result in undefined expressions since <span class="math inline">\(\log(0)\)</span> is undefined and so is <span class="math inline">\(\frac{1}{0}\)</span>.</p>
<p>Exponentiating both sides produces an equation modeling the odds:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{p}{1-p} &amp;= e^{-4.93 + 0.003\ \textrm{gre} + 0.755\ \textrm{gpa}}\\
&amp;= e^{-4.93}e^{0.003\ \textrm{gre}}e^{0.755\ \textrm{gpa}}\\
&amp;=(0.007) 1.003^{\textrm{gre}}2.127^{\textrm{gpa}}
\end{aligned}
\]</span> Increasing <span class="math inline">\(\textrm{gre}\)</span> by 1 will multiply the RHS by 1.003. In other words it increases the odds by a factor of 1.003. The effect size might not be high… but a GRE score can vary quite a bit… increasing it by 100 increases the odds by <span class="math inline">\(1.003^{100}=1.349\)</span>. GPA on the other hand, which has a much more limited range, has a larger effect on a <strong>per-point</strong> basis (whether or not such a distinction is important I will leave for you to consider).</p>
<p>Notice that <span class="math inline">\(1+odds = \frac{1}{1-p}\)</span> <span class="math display">\[
\begin{aligned}
\frac{odds}{1+odds}&amp;= \frac{p}{1-p}\frac{1}{1+\frac{p}{1-p}}\\
&amp;=\frac{p}{1-p}\frac{1}{\frac{1-p}{1-p}+\frac{p}{1-p}}\\
&amp;=\frac{p}{1-p}\frac{1}{\frac{1}{1-p}}\\
&amp;=\frac{p}{1-p}\frac{p-1}{1}\\
&amp;=p
\end{aligned}
\]</span></p>
<p>So although we can turn information about the odds into information about probability… a straightforward interpretation is still a bit problematics.</p>
<p>Let’s train our intuition a bit by looking at just gpa:</p>
<p><img src="Linear_Regression_files/figure-html/unnamed-chunk-12-1.png" width="624" /></p>
<p>The y-axis is quite boring– just 0/1. But by using some jitter and transparency to deal with overplotting we see that an increase in GPA really does seem to improve the probability of being accepted.</p>
<p>The model of this is</p>
<pre><code>## 
## Call:
## glm(formula = admit ~ gpa, family = &quot;binomial&quot;, data = df)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1131  -0.8874  -0.7566   1.3305   1.9824  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -4.3576     1.0353  -4.209 2.57e-05 ***
## gpa           1.0511     0.2989   3.517 0.000437 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 486.97  on 398  degrees of freedom
## AIC: 490.97
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>So</p>
<p><span class="math display">\[
\begin{aligned}
odds&amp;=(0.0128)2.8608^{\textrm{gpa}}\\
\frac{odds}{1+odds} &amp;= \frac{(0.0128)2.8608^{\textrm{gpa}}}{1+(0.0128)2.8608^{\textrm{gpa}}}\\
p&amp;=\frac{1}{(78.0686)2.8608^{-\textrm{gpa}} +1}
\end{aligned}
\]</span> <img src="Linear_Regression_files/figure-html/unnamed-chunk-14-1.png" width="624" /></p>
<p>That’s not too shabby. If you have a 4.0 GPA your probability of acceptance is about 46% and that’s what the model predicts.</p>
<pre><code>## 
##  0  1 
## 15 13</code></pre>
<pre><code>## [1] 0.4642857</code></pre>
<pre><code>## [1] 0.461779</code></pre>
<p><strong>Exercise:</strong> Repeat the process above but use <span class="math inline">\(\textrm{gre}\)</span> as the explanatory variable. Produce the model and graph the logistic curve for <span class="math inline">\(p\)</span> with explanatory variable <span class="math inline">\(\textrm{gre}\)</span>. Check the prediction for <span class="math inline">\(\textrm{gre}=800\)</span> against the actual sample probability.</p>
<p>NOTE: Up above I solved the equation for <span class="math inline">\(p\)</span> algebraically but sometimes it is easier to just let the computer do it for us. The expression <code>fitted.value(model)</code> will generate the fitted values. I will redo the logistic regression using <span class="math inline">\(\textrm{gpa}\)</span> and <span class="math inline">\(\textrm{gre}\)</span>, then produce a formula that will convert the explanatory values to the fitted values. We can then compare the output of <code>fitted.values()</code> to our results. Notice that we have <strong>two</strong> explanatory variables… so we’ll need to be a bit fancier in our graphing. I will use the graphing package <code>plotly</code></p>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## 
## Attaching package: &#39;plotly&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     last_plot</code></pre>
<pre><code>## The following object is masked from &#39;package:MASS&#39;:
## 
##     select</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     filter</code></pre>
<pre><code>## The following object is masked from &#39;package:graphics&#39;:
## 
##     layout</code></pre>
<p><div id="htmlwidget-671919dbb126ef712d61" style="width:624px;height:384px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-671919dbb126ef712d61">{"x":{"visdat":{"9f83ff0e9ec":["function () ","plotlyVisDat"]},"cur_data":"9f83ff0e9ec","attrs":{"9f83ff0e9ec":{"size":0.2,"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"GPA"},"yaxis":{"title":"GRE"},"zaxis":{"title":"Probability"}},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[3.61,3.67,4,3.19,2.93,3,2.98,3.08,3.39,3.92,4,3.22,4,3.08,4,3.44,3.87,2.56,3.75,3.81,3.17,3.63,2.82,3.19,3.35,3.66,3.61,3.74,3.22,3.29,3.78,3.35,3.4,4,3.14,3.05,3.25,2.9,3.13,2.68,2.42,3.32,3.15,3.31,2.94,3.45,3.46,2.97,2.48,3.35,3.86,3.13,3.37,3.27,3.34,4,3.19,2.94,3.65,2.82,3.18,3.32,3.67,3.85,4,3.59,3.62,3.3,3.69,3.73,4,2.92,3.39,4,3.45,4,3.36,4,3.12,4,2.9,3.07,2.71,2.91,3.6,2.98,3.32,3.48,3.28,4,3.83,3.64,3.9,2.93,3.44,3.33,3.52,3.57,2.88,3.31,3.15,3.57,3.33,3.94,3.95,2.97,3.56,3.13,2.93,3.45,3.08,3.41,3,3.22,3.84,3.99,3.45,3.72,3.7,2.92,3.74,2.67,2.85,2.98,3.88,3.38,3.54,3.74,3.19,3.15,3.17,2.79,3.4,3.08,2.95,3.57,3.33,4,3.4,3.58,3.93,3.52,3.94,3.4,3.4,3.43,3.4,2.71,2.91,3.31,3.74,3.38,3.94,3.46,3.69,2.86,2.52,3.58,3.49,3.82,3.13,3.5,3.56,2.73,3.3,4,3.24,3.77,4,3.62,3.51,2.81,3.48,3.43,3.53,3.37,2.62,3.23,3.33,3.01,3.78,3.88,4,3.84,2.79,3.6,3.61,2.88,3.07,3.35,2.94,3.54,3.76,3.59,3.47,3.59,3.07,3.23,3.63,3.77,3.31,3.2,4,3.92,3.89,3.8,3.54,3.63,3.16,3.5,3.34,3.02,2.87,3.38,3.56,2.91,2.9,3.64,2.98,3.59,3.28,3.99,3.02,3.47,2.9,3.5,3.58,3.02,3.43,3.42,3.29,3.28,3.38,2.67,3.53,3.05,3.49,4,2.86,3.45,2.76,3.81,2.96,3.22,3.04,3.91,3.34,3.17,3.64,3.73,3.31,3.21,4,3.55,3.52,3.35,3.3,3.95,3.51,3.81,3.11,3.15,3.19,3.95,3.9,3.34,3.24,3.64,3.46,2.81,3.95,3.33,3.67,3.32,3.12,2.98,3.77,3.58,3,3.14,3.94,3.27,3.45,3.1,3.39,3.31,3.22,3.7,3.15,2.26,3.45,2.78,3.7,3.97,2.55,3.25,3.16,3.07,3.5,3.4,3.3,3.6,3.15,3.98,2.83,3.46,3.17,3.51,3.13,2.98,4,3.67,3.77,3.65,3.46,2.84,3,3.63,3.71,3.28,3.14,3.58,3.01,2.69,2.7,3.9,3.31,3.48,3.34,2.93,4,3.59,2.96,3.43,3.64,3.71,3.15,3.09,3.2,3.47,3.23,2.65,3.95,3.06,3.35,3.03,3.35,3.8,3.36,2.85,4,3.43,3.12,3.52,3.78,2.81,3.27,3.31,3.69,3.94,4,3.49,3.14,3.44,3.36,2.78,2.93,3.63,4,3.89,3.77,3.76,2.42,3.37,3.78,3.49,3.63,4,3.12,2.7,3.65,3.49,3.51,4,2.62,3.02,3.86,3.36,3.17,3.51,3.05,3.88,3.38,3.75,3.99,4,3.04,2.63,3.65,3.89],"y":[380,660,800,640,520,760,560,400,540,700,800,440,760,700,700,480,780,360,800,540,500,660,600,680,760,800,620,520,780,520,540,760,600,800,360,400,580,520,500,520,560,580,600,500,700,460,580,500,440,400,640,440,740,680,660,740,560,380,400,600,620,560,640,680,580,600,740,620,580,800,640,300,480,580,720,720,560,800,540,620,700,620,500,380,500,520,600,600,700,660,700,720,800,580,660,660,640,480,700,400,340,580,380,540,660,740,700,480,400,480,680,420,360,600,720,620,440,700,800,340,520,480,520,500,720,540,600,740,540,460,620,640,580,500,560,500,560,700,620,600,640,700,620,580,580,380,480,560,480,740,800,400,640,580,620,580,560,480,660,700,600,640,700,520,580,700,440,720,500,600,400,540,680,800,500,620,520,620,620,300,620,500,700,540,500,800,560,580,560,500,640,800,640,380,600,560,660,400,600,580,800,580,700,420,600,780,740,640,540,580,740,580,460,640,600,660,340,460,460,560,540,680,480,800,800,720,620,540,480,720,580,600,380,420,800,620,660,480,500,700,440,520,680,620,540,800,680,440,680,640,660,620,520,540,740,640,520,620,520,640,680,440,520,620,520,380,560,600,680,500,640,540,680,660,520,600,460,580,680,660,660,360,660,520,440,600,800,660,800,420,620,800,680,800,480,520,560,460,540,720,640,660,400,680,220,580,540,580,540,440,560,660,660,520,540,300,340,780,480,540,460,460,500,420,520,680,680,560,580,500,740,660,420,560,460,620,520,620,540,660,500,560,500,580,520,500,600,580,400,620,780,620,580,700,540,760,700,720,560,720,520,540,680,460,560,480,460,620,580,800,540,680,680,620,560,560,620,800,640,540,700,540,540,660,480,420,740,580,640,640,800,660,600,620,460,620,560,460,700,600],"z":[0.231031001744156,0.400393419642082,0.5552524946053,0.305787118456092,0.207676184520981,0.34515658512593,0.232610624256175,0.175278573463408,0.281300369949826,0.473144103311758,0.5552524946053,0.208269918468776,0.528541163361081,0.32268408390397,0.488214153165065,0.256979532070702,0.517491178425624,0.11418040480985,0.508310365633005,0.349542228371639,0.229404118237068,0.393168328812054,0.230276433960125,0.329099181249597,0.407029843718026,0.491331545387859,0.364303327156001,0.325701021679286,0.396384386224786,0.255916348369158,0.344412298597909,0.407029843718026,0.31668841258569,0.5552524946053,0.166447708513135,0.172029761859958,0.281699056893639,0.203975396935033,0.224111277939444,0.178336047218562,0.165722267807886,0.292510708973792,0.277338065860719,0.248613110214702,0.300037312334639,0.248245066408828,0.314844909501203,0.20381541360876,0.130805676913139,0.206704810556146,0.422077873938222,0.19729179814449,0.397719213433676,0.342564387589517,0.342342452899831,0.515114199471104,0.262088014061628,0.153405259684464,0.246290069659776,0.230276433960125,0.292918808724109,0.281499670492685,0.387545878073814,0.446661517556154,0.408532768997181,0.348499335710197,0.443663784288165,0.312021045622102,0.353432043699302,0.504537572147663,0.448039982289879,0.125814129496776,0.249840807648148,0.408532768997181,0.399291942506092,0.501665381422244,0.287645387043895,0.5552524946053,0.241992612978495,0.434772153479464,0.293736007752594,0.276024966979083,0.173813897014307,0.150487894514831,0.291695510921263,0.213953750137398,0.303770356612323,0.329895519107691,0.356514023656527,0.461382199077108,0.456249948000361,0.434128679228451,0.536546593885253,0.235493162684866,0.359530127601815,0.340645347292877,0.361042369392653,0.276154423909709,0.290614511419381,0.201798542854564,0.160127187721328,0.33302177939735,0.19563364704684,0.372164387923926,0.452019642090937,0.328086517519368,0.406316403100899,0.214892950567236,0.159510844976206,0.258423179955172,0.311036569569351,0.223426503335447,0.152300405328156,0.28804946253509,0.471508810442013,0.432918467094454,0.238338985026133,0.435741219941146,0.4988775471885,0.138135852122746,0.325701021679286,0.162079532663471,0.197916887237617,0.205042817118591,0.479037077582969,0.279777137293539,0.339981550630133,0.466116173301333,0.251814572773652,0.208432482970707,0.291358167479209,0.245684183089131,0.305159810719044,0.217618192717435,0.228593699469643,0.287039907936177,0.28302859599156,0.488214153165065,0.328446676266869,0.346787802717444,0.435014373661532,0.399055563972295,0.423679166886027,0.305159810719044,0.305159810719044,0.207782785694605,0.251257913263959,0.198229990745082,0.188205743332316,0.386924452838551,0.506424060345317,0.210442027153116,0.436870120703078,0.314844909501203,0.378396525257907,0.226115419167753,0.176421279122595,0.277665536900706,0.368264407388496,0.454378302928297,0.274323143755732,0.357567737075146,0.406316403100899,0.183932613133107,0.289396898247905,0.488214153165065,0.210769738500333,0.458367509068307,0.357716541579618,0.353657301570625,0.227206292195067,0.201694230198036,0.379092245857249,0.448123538430585,0.280902026603438,0.323472383154068,0.171797244169929,0.300794856434041,0.316901730124935,0.133476297550359,0.39449940404938,0.337187885703294,0.488214153165065,0.354707225854654,0.182655331162689,0.480021017176304,0.327794832907509,0.228767543891763,0.244954302516979,0.254294956774841,0.267261519709006,0.468730931626665,0.403785192449466,0.228360406287124,0.328229321542351,0.324477682637755,0.298042191539673,0.192248434191965,0.355384292716932,0.36734770956559,0.425846533462655,0.274126985645142,0.488214153165065,0.297147485225357,0.401495904435241,0.504291193358189,0.42881667244734,0.380405539766183,0.247573007074388,0.32139298107042,0.392308673822582,0.247940383119458,0.175706403621579,0.337043139579023,0.343376631727833,0.273410341384654,0.136348672529043,0.275957454283353,0.188055206579732,0.324477682637755,0.264825642950804,0.47289842268166,0.201218568739841,0.455600308434677,0.352460941122464,0.40837626616644,0.359076352373579,0.228419948027975,0.255541169893048,0.39387399950252,0.287847382544227,0.297423965052927,0.201639834402791,0.141331219904754,0.466852045776326,0.273018926586573,0.368264407388496,0.345449445790317,0.190674738011528,0.386456964903751,0.156759217950463,0.337408193177408,0.291969025236882,0.299210009475248,0.23109103127068,0.538422704664547,0.354558982133235,0.202116239716713,0.407900104227521,0.398346710953274,0.337263400602705,0.2976299583766,0.370172894082512,0.306343346510065,0.42512375699275,0.332003018575792,0.25735609228314,0.42552297775398,0.288789226946668,0.412901476345965,0.315908925864672,0.19969308513566,0.241811869656729,0.42552297775398,0.352756660832104,0.196823959272904,0.26945098328505,0.357115057625536,0.375546001210159,0.184919513073904,0.438727636856984,0.272237147573663,0.413379400542536,0.338952318738933,0.232258947625579,0.252372049868183,0.295984317330831,0.33470018391133,0.298248431277049,0.309211078873101,0.450150984221297,0.180508530886581,0.36126976750229,0.229578395555795,0.230216552682246,0.302176607521856,0.409329105294406,0.405840995372014,0.39662022678251,0.107772496458871,0.33682294929577,0.332077954063941,0.418880229531839,0.54965469784281,0.150153396243217,0.250210429844835,0.257733012882161,0.198646555589293,0.298384015344278,0.390276392961374,0.323688105651905,0.387779838259052,0.183046639194961,0.471017642289232,0.0978218649808182,0.314844909501203,0.248981518954653,0.323041163378575,0.243379645767436,0.179975948319851,0.3955963670196,0.400393419642082,0.418640336337011,0.310964171621787,0.292102942912134,0.119322423628548,0.145482015857875,0.472245665824151,0.297765380585785,0.264825642950804,0.207190077044022,0.26700242278291,0.208757892350326,0.143172881275903,0.180558515393032,0.456005447051666,0.349394974481834,0.306552821917669,0.295644084195315,0.198960527090931,0.515114199471104,0.385989686182948,0.170029549097697,0.2985903910027,0.275957454283353,0.381953222952928,0.236320548283626,0.279051400487977,0.253239094350542,0.364759948579421,0.237506730437864,0.191131438791466,0.349094188154308,0.253612044381892,0.264633799547847,0.211262006559135,0.308579869275631,0.372625086888039,0.207945069832493,0.244106351302946,0.541926955345942,0.333459759402719,0.262278670162584,0.399055563972295,0.344412298597909,0.313503960171126,0.354784564135608,0.374239350813008,0.34123381536675,0.490346284570446,0.370172894082512,0.296806478722968,0.320822065714279,0.246839351399396,0.287645387043895,0.173672408912412,0.182361232515574,0.367805939041064,0.408532768997181,0.534669447916346,0.342710279115799,0.429941302768329,0.21528277626621,0.323472383154068,0.356662633100652,0.308159459508409,0.367805939041064,0.5552524946053,0.294689146893903,0.188657908931984,0.422799280501638,0.296806478722968,0.299966359921104,0.461382199077108,0.157019956746045,0.176515411159689,0.488706686639447,0.298796848611697,0.302592425033359,0.359303208525277,0.378707869652617,0.4389703492846,0.313431249180778,0.389104368101421,0.331709570976042,0.434772153479464,0.240791101959621,0.150992586683518,0.422799280501638,0.401495904435241],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","size":[55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55],"sizemode":"area","line":{"color":"rgba(31,119,180,1)"}},"textfont":{"size":55},"error_y":{"color":"rgba(31,119,180,1)","width":55},"error_x":{"color":"rgba(31,119,180,1)","width":55},"line":{"color":"rgba(31,119,180,1)","width":55},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script> Now let’s see if we correctly understand the equation. First we get the coefficients from the model:</p>
<pre><code>##  (Intercept)          gpa          gre 
## -4.949378063  0.754686856  0.002690684</code></pre>
<p>These three values fit into the equation:</p>
<p><span class="math display">\[
\log\left(\frac{p}{1-p}\right)=-4.944 + 0.755\textrm{ gpa}+0.003\textrm{ gre}
\]</span></p>
<p>The left hand side is the <em>log odds</em> (aka logit). Let’s capture this in an equation. Make sure you understand how my expression matches the RHS of the equation.</p>
<p>Now let’s turn this into the actual probabilities:</p>
<p>We can use <code>apply</code> to apply this function to every row in the data.frame to compare our values to the fitted values:</p>
<pre><code>##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -1.110e-16  0.000e+00  0.000e+00 -1.353e-18  0.000e+00  1.110e-16</code></pre>
<p>Notice how close our values are… just a few rounding issues (<code>-1.110e-16</code> and <code>1.110e-16</code> are VERY close to 0).</p>
<p>So… success… we understand how to do this.</p>
<p>Now <strong>you</strong> do the same thing, but with the full model. Be sure to put your summary into a variable called <code>my.summary</code></p>
<p>://www.statisticalassociates.com/logistic10.htm</p>
<p><a href="http://www.statisticalassociates.com/booklist.htm" class="uri">http://www.statisticalassociates.com/booklist.htm</a></p>
<p>Look at the first few lines of the table:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
admit
</th>
<th style="text-align:right;">
gre
</th>
<th style="text-align:right;">
gpa
</th>
<th style="text-align:right;">
rank
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
380
</td>
<td style="text-align:right;">
3.61
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
1
</td>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
660
</td>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
3.67
</td>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
3
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
1
</td>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
800
</td>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
4.00
</td>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
1
</td>
</tr>
<tr>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
1
</td>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
640
</td>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
3.19
</td>
<td style="text-align:right;font-weight: bold;background-color: #CAEFCA !important;">
4
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
520
</td>
<td style="text-align:right;">
2.93
</td>
<td style="text-align:right;">
4
</td>
</tr>
</tbody>
</table>
<p>I have highlighted the rows that were associated to admitted individuals.</p>
<p>Let’s build a full model (using <code>rank</code> as a factor()) and predict “admit” and make some predictions about admittance using our model. The idea is that we let the explanatory variables predict the probability.</p>
<p>Some people use logistic regression for classification. <a href="https://www.fharrell.com/post/classification/">There are good arguments against this</a> (you should follow and read that link).</p>
<p>But, that said, we could try it anyway… we need to make some decision… will we call a row isn the table a “yes” or a “no”. One approach would be to say that anything <em>at or above</em> 0.5 counts as a yes(1). And anything below counts as a no (0).</p>
<p>Let’s build the full model and make our predictions:</p>
<pre><code>##    predicted.cat
##       0   1
##   0 254  19
##   1  97  30</code></pre>
<p>The rows are the actual values, and the cols are the predicted values. This table is known as the <strong>confusion matrix</strong> (because the off-diagonals measure the mistakes)</p>
<p>You can see that for 254 cases our algorithm predicted “no” and that was the result. In 30 cases, our algorithm predicted “yes” and that was the result. In 19 cases our algorithm predicted “yes” when the true situations was “no”. And the most extreme situation mistakes occurred when we predicted “no”, but the true situation wasy “yes”</p>
<p>Each of the squares in the table have names:</p>
<ul>
<li><strong>True Positives</strong>: The observed and predicted value were both 1 (30 of these for us)</li>
<li><strong>True Negatives</strong>: The observed and predicted value were both 0 (254 of these for us)</li>
<li><strong>False Positives</strong>: Prediction was “yes”, but observed was “no” (19 of these for us)</li>
<li><strong>False Negatives</strong>: Prediction was “no”, but observed was “yes” (97 of these for us)</li>
</ul>
<p>False Positives are also known as <strong>type I errors</strong> and False Negatives are also known as <strong>type II errors</strong>. (Think about that for a bit in the context of what you learned in Introduction to Statistics and Hypothesis Tests)</p>
<p>Make the model in the console use <span class="math inline">\(\textrm{gpa}\)</span> and <span class="math inline">\(\textrm{gre}\)</span> as the predictors. Use 0.5 as the prediction threshold. Answer the following questions</p>
<div class="panel panel-default">
<div class="panel-heading tutorial-panel-heading">Quiz</div>
<table class="table quiz-table">
<tr>
<td>

<div id="htmlwidget-82f6379be78197dbb102" style="width:100%;height:auto;", class = "quiz html-widget">
<div class="panel panel-default">
<div class="panel-body quizArea">
</div>
</div>
</div>

<script type="application/json" data-for="htmlwidget-82f6379be78197dbb102">{"x":{"question":"The number of true positives","answers":[{"option":"263","correct":false,"message":null},{"option":"10","correct":false,"message":null},{"option":"118","correct":false,"message":null},{"option":"9","correct":true,"message":null}],"label":"ML-quiz-1","skipStartButton":true,"perQuestionResponseAnswers":true,"perQuestionResponseMessaging":true,"preventUnanswered":true,"displayQuestionCount":false,"displayQuestionNumber":false,"disableRanking":true,"nextQuestionText":"","checkAnswerText":"Submit Answer","allowRetry":false,"randomSortAnswers":false,"json":{"info":{"name":"","main":""},"questions":[{"q":"The number of true positives","a":[{"option":"263","correct":false,"message":null},{"option":"10","correct":false,"message":null},{"option":"118","correct":false,"message":null},{"option":"9","correct":true,"message":null}],"correct":"Correct!","incorrect":"Incorrect."}]}},"evals":[],"jsHooks":[]}</script>
</td>
</tr>
<tr>
<td>

<div id="htmlwidget-ba9726661f2b3cab670c" style="width:100%;height:auto;", class = "quiz html-widget">
<div class="panel panel-default">
<div class="panel-body quizArea">
</div>
</div>
</div>

<script type="application/json" data-for="htmlwidget-ba9726661f2b3cab670c">{"x":{"question":"The number of false positives","answers":[{"option":"263","correct":false,"message":null},{"option":"10","correct":true,"message":null},{"option":"118","correct":false,"message":null},{"option":"9","correct":false,"message":null}],"label":"ML-quiz-2","skipStartButton":true,"perQuestionResponseAnswers":true,"perQuestionResponseMessaging":true,"preventUnanswered":true,"displayQuestionCount":false,"displayQuestionNumber":false,"disableRanking":true,"nextQuestionText":"","checkAnswerText":"Submit Answer","allowRetry":false,"randomSortAnswers":false,"json":{"info":{"name":"","main":""},"questions":[{"q":"The number of false positives","a":[{"option":"263","correct":false,"message":null},{"option":"10","correct":true,"message":null},{"option":"118","correct":false,"message":null},{"option":"9","correct":false,"message":null}],"correct":"Correct!","incorrect":"Incorrect."}]}},"evals":[],"jsHooks":[]}</script>
</td>
</tr>
<tr>
<td>

<div id="htmlwidget-8e917e1d66342b45a80a" style="width:100%;height:auto;", class = "quiz html-widget">
<div class="panel panel-default">
<div class="panel-body quizArea">
</div>
</div>
</div>

<script type="application/json" data-for="htmlwidget-8e917e1d66342b45a80a">{"x":{"question":"The number of true negatives","answers":[{"option":"263","correct":true,"message":null},{"option":"10","correct":false,"message":null},{"option":"118","correct":false,"message":null},{"option":"9","correct":false,"message":null}],"label":"ML-quiz-3","skipStartButton":true,"perQuestionResponseAnswers":true,"perQuestionResponseMessaging":true,"preventUnanswered":true,"displayQuestionCount":false,"displayQuestionNumber":false,"disableRanking":true,"nextQuestionText":"","checkAnswerText":"Submit Answer","allowRetry":false,"randomSortAnswers":false,"json":{"info":{"name":"","main":""},"questions":[{"q":"The number of true negatives","a":[{"option":"263","correct":true,"message":null},{"option":"10","correct":false,"message":null},{"option":"118","correct":false,"message":null},{"option":"9","correct":false,"message":null}],"correct":"Correct!","incorrect":"Incorrect."}]}},"evals":[],"jsHooks":[]}</script>
</td>
</tr>
<tr>
<td>

<div id="htmlwidget-a2f17b916d7dae576760" style="width:100%;height:auto;", class = "quiz html-widget">
<div class="panel panel-default">
<div class="panel-body quizArea">
</div>
</div>
</div>

<script type="application/json" data-for="htmlwidget-a2f17b916d7dae576760">{"x":{"question":"The number of false negatives","answers":[{"option":"263","correct":false,"message":null},{"option":"10","correct":false,"message":null},{"option":"118","correct":true,"message":null},{"option":"9","correct":false,"message":null}],"label":"ML-quiz-4","skipStartButton":true,"perQuestionResponseAnswers":true,"perQuestionResponseMessaging":true,"preventUnanswered":true,"displayQuestionCount":false,"displayQuestionNumber":false,"disableRanking":true,"nextQuestionText":"","checkAnswerText":"Submit Answer","allowRetry":false,"randomSortAnswers":false,"json":{"info":{"name":"","main":""},"questions":[{"q":"The number of false negatives","a":[{"option":"263","correct":false,"message":null},{"option":"10","correct":false,"message":null},{"option":"118","correct":true,"message":null},{"option":"9","correct":false,"message":null}],"correct":"Correct!","incorrect":"Incorrect."}]}},"evals":[],"jsHooks":[]}</script>
</td>
</tr>
</table>
</div>
<p>It’s not too surprising that the number of false negatives is the highest category. If you <strong>ignored every predictor</strong> your best guess would be “no”. And this simple algorithm woudl give you the proper predictor 68.25% of the time!</p>
<p>Let’s call this the <strong>accuracy</strong>. In other words the number of predictions that were correct.</p>
<p><span class="math display">\[
\textrm{Accuracy} = \frac{\textrm{True Positives}+\textrm{True Negatives}}{\textrm{Total}}
\]</span></p>
<p>So… in our example the “always guess <em>no</em>” approach has an accuracy of over 68%. Notice that the logistic regression approach from your last exercise (using a threshold of 0.5) has an accuracy of <span class="math inline">\(\frac{263+9}{400} = 68%\)</span>. Hmmm… that wasn’t very satisfying was it? Even including the <code>rank</code> (as a factor) only changes the accuracy to 71%</p>
<p>There does seem to be something fundamentally different about the two strategies… The “always say no” approach will never yield a false positive (nor a true positive). The logistic regression prediction approach is a bit more balanced.</p>
<p>Let’s see what effect changing our threshold has. Let’s go back to the full model and make an accuracy curve:</p>
<p><img src="Linear_Regression_files/figure-html/unnamed-chunk-23-1.png" width="624" /></p>
<pre><code>## [1] 0.49</code></pre>
<pre><code>## [1] 0.715</code></pre>
<p>We can get up to 71.5% if we use 49% as a threshold… lets’ look a bit closer:</p>
<p><img src="Linear_Regression_files/figure-html/unnamed-chunk-24-1.png" width="624" /></p>
<pre><code>## [1] 0.486</code></pre>
<pre><code>## [1] 0.7175</code></pre>
<p>A very minor improvement by changing the threshold to 48.6%… still that’s pretty strange.</p>
<p>Depending upon your purposes you might be willing to have fewer accurate predictions as long as you have fewer false positives… take, for example, a spam filter (which we’ll discuss below in the context of Naive Bayes and the bag of words model).</p>
<p>It is far better for some real spam slip into a persons inbox then for a real communication to be classified as spam and filtered. In that situation, we might use logistic regression and set a threshold of 0.9.</p>
<p>in that situation we would be far more concerned with the <strong>precision</strong>– that’s the percentage of things classified as “yes” that are correct:</p>
<p><span class="math display">\[
\textrm{Precision} = \frac{\textrm{True Positives}}{\textrm{True Positives}+\textrm{False Positives}}
\]</span> Another useful measure is <strong>recall</strong> (also known as <strong>sensitivity</strong> also known as <strong>true positive rate</strong>)- this measures the percentage of the <strong>actual positives</strong> that were labeled correctly</p>
<p><span class="math display">\[
\textrm{Recall} = \frac{\textrm{True Positives}}{\textrm{True Positives}+\textrm{False Negatives}}
\]</span></p>
<p>Read that a couple of times (it always confuses me the first time I review the material). THe true positives are the cases labeled positive that <strong>are</strong> positive. The False negatives are the mislabeled <strong>actual positives</strong>. So the recall (or sensitivity) is a measure of how many of the positives are identified (ignoring false positives).</p>
<p>It’s easy to get a high recall– just adopt the “always say yes” strategy. Then EVERYTHING is a positive… and so the recall is always 100%.</p>
<p>We also have <strong>specificity</strong> (also known as the <strong>true negative rate</strong>)</p>
<p><span class="math display">\[
\textrm{Specificity} = \frac{\textrm{True Negatives}}{\textrm{True Negatives}+\textrm{False Positives}}
\]</span></p>
<p>Let’s think of the possible results that could occur for test that tries to detect a disease:</p>
<table>
<thead>
<tr class="header">
<th>metric</th>
<th>interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>P(test result is correct)</td>
</tr>
<tr class="even">
<td>Sensitivity</td>
<td>P(result is positive</td>
</tr>
<tr class="odd">
<td>Specificity</td>
<td>P(result is negative</td>
</tr>
</tbody>
</table>
<p>There’s one other common measure you might encounter… It’s called <span class="math inline">\(F1\)</span> and it tries to balance the true positive rate and the true negative rate.</p>
<p><span class="math display">\[
\textrm{F1} = 2\left(\frac{\textrm{Precision}*\textrm{Recall}}{\textrm{Precision}+\textrm{Recall}}\right)
\]</span> Notice that if our accuracy was <strong>perfect</strong> then <span class="math inline">\(\textrm{Precision}\)</span> and <span class="math inline">\(\textrm{Recall}\)</span> would both be 1. In this situation <span class="math inline">\(F1\)</span> would be 1. If either <span class="math inline">\(\textrm{Precision}\)</span> and <span class="math inline">\(\textrm{Recall}\)</span> is 0, then the numerator is 0 and the denominator is <strong>not 0</strong> (they can’t both be 0 at the same time). This is actually (after some algebraic manipualtion) the <strong>harmonic mean</strong> of <span class="math inline">\(\textrm{precision}\)</span> and <span class="math inline">\(\textrm{recall}\)</span>.</p>
</div>
<div id="section-decision-trees" class="section level2">
<h2>Decision Trees</h2>
<p>Perhaps a better way to make a decision is to use something more suited to classification… such as a decision tree read <a href="https://medium.com/analytics-vidhya/a-guide-to-machine-learning-in-r-for-beginners-decision-trees-c24dfd490abb">this article</a> now.</p>
<p>We already have the data in the <code>df</code> dataframe:</p>
<p><img src="Linear_Regression_files/figure-html/unnamed-chunk-25-1.png" width="624" /></p>
<p>So, given a row in the table we follow the directions by answering a series of yes/no questions:</p>
<p>Let’s use the first row as an example.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\textrm{gpa}&lt;3.4\)</span> NO (go right)</li>
<li><span class="math inline">\(\textrm{rank}\)</span> is 2, 3, or 4. YES (go left)</li>
<li>Prediction is 0</li>
</ol>
<p>Just like the results from the logistic regression we can use the model to make a prediction.</p>
<p>First look at the first few predictions:</p>
<pre><code>##           0         1
## 1 0.6966292 0.3033708
## 2 0.6966292 0.3033708
## 3 0.2500000 0.7500000
## 4 0.8686869 0.1313131
## 5 0.8686869 0.1313131
## 6 0.3000000 0.7000000</code></pre>
<p>You can see that the tree puts 70% chance that row 1 is a 0 and a 30% chance that it is a 1. We want to pick the larger of the two values. We can, fairly easily, look at the column for 1. If it is higher than 0.5 then we’ll choose 1 (since each row must add to 1). I choose higher than 0.5 instead of higher than <em>or equal</em> to 0.5 since there are more 0’s… so if we don’t know any better, might as well choose the more common one. Note that the innermost expression evaluates to <code>TRUE</code> and <code>FALSE</code>, but <code>as.numeric</code> turns <code>FALSE</code> to 0 and <code>TRUE</code> to 1:</p>
<p>Let’s make the confusion matrix for this classification:</p>
<pre><code>##    pv
##       0   1
##   0 249  24
##   1  73  54</code></pre>
<p>Our accuracy is <span class="math inline">\(\frac{249+54}{400} = 0.7575\)</span>. That’s better than before. Also our Type I and Type II error rates are more balanced.</p>
Notice that <strong>it is impossible</strong> to guess everything correctly. Rows 15 and 166 on the table are identical in all respects <strong>except</strong> the value of <em>admit</em>:
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
admit
</th>
<th style="text-align:right;">
gre
</th>
<th style="text-align:right;">
gpa
</th>
<th style="text-align:left;">
rank
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
15
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
700
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
166
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
700
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
1
</td>
</tr>
</tbody>
</table>
<p>Now… MOST of the combinations of values are distinct… which means that one <strong>could</strong>, produce a very exhaustive list of special cases to make VERY accurate predictions… HOWEVER that would only work on the specific data-set and would be extremely unlikely to transfer well to new data. This problem is known as <strong>over-fitting</strong>.</p>
<p>However… look at the tree again and think about what it’s saying… The first thing it looks at is a student’s gpa. Is it low (below 3.4) or high? That’s the first devision. If the GPA is low than a student who does not come from a well-ranked school is weeded out. If they DO come from a higher ranked school then they’d better have a pretty impressive GRE score. A score of less than 730 rules them out… otherwise they’re in. (probabilistically speaking of course)</p>
<p>On the other hand.. if the GPA is higher and the school is top ranked… then the student is accepted if the school is low ranked 3 or 4 the student is rejected. After this the GPA is taken into consideration. This one gets a bit… <em>weird</em> If your GPA iS between 3.4 and 3.5 then you’re accepted. If your GPA is between 3.5 and 3.7 you are rejected. Otherwise your GRE is used to determine your acceptance.</p>
<p>Let’s add <code>extra=1</code> to the <code>prp</code> function. This will add the number of rows correctly (and incorrectly) classified:</p>
<p><img src="Linear_Regression_files/figure-html/unnamed-chunk-29-1.png" width="624" /></p>
<p>This provides us a better sense from whence the errors in prediction are arising…. first off– notice that no matter what occurs at the first split– a rank 3 or rank 4 school is given a best guess of 0… so in that subset… without taking anything else into consideration just saying predicting “reject” will be correct almost 79% of the time… So that’s a pretty good indicator. It’s not that the tree couldn’t decide to be more aggressive in its branching… but that defeats part of the purpose– the goal is to identify <em>trends</em>– simplifying patterns that tell us something meaningful about the overall system– not to blindly pursue accuracy at the cost of sensibiity.</p>
<p>The <code>summary</code> provides another way to view the rules. Note that some nodes may be “collapsed” and not appear in the graphed tree:</p>
<div style="max-height: 300px;float: left;width: 100%;overflow-y: auto;">
<pre><code>## Call:
## rpart(formula = admit ~ gre + gpa + rank, data = df, method = &quot;class&quot;)
##   n= 400 
## 
##           CP nsplit rel error    xerror       xstd
## 1 0.06299213      0 1.0000000 1.0000000 0.07330768
## 2 0.02362205      2 0.8740157 0.9133858 0.07145859
## 3 0.01574803      4 0.8267717 1.0000000 0.07330768
## 4 0.01000000      8 0.7637795 1.0629921 0.07446567
## 
## Variable importance
##  gpa rank  gre 
##   42   36   22 
## 
## Node number 1: 400 observations,    complexity param=0.06299213
##   predicted class=0  expected loss=0.3175  P(node) =1
##     class counts:   273   127
##    probabilities: 0.682 0.318 
##   left son=2 (208 obs) right son=3 (192 obs)
##   Primary splits:
##       gpa  &lt; 3.415 to the left,  improve=8.867821, (0 missing)
##       rank splits as  RRLL,      improve=7.781937, (0 missing)
##       gre  &lt; 510   to the left,  improve=4.843976, (0 missing)
##   Surrogate splits:
##       gre  &lt; 610   to the left,  agree=0.645, adj=0.260, (0 split)
##       rank splits as  RLRL,      agree=0.540, adj=0.042, (0 split)
## 
## Node number 2: 208 observations,    complexity param=0.01574803
##   predicted class=0  expected loss=0.2163462  P(node) =0.52
##     class counts:   163    45
##    probabilities: 0.784 0.216 
##   left son=4 (99 obs) right son=5 (109 obs)
##   Primary splits:
##       rank splits as  RRLL,      improve=2.731978, (0 missing)
##       gre  &lt; 750   to the left,  improve=2.515925, (0 missing)
##       gpa  &lt; 3.235 to the right, improve=1.053296, (0 missing)
##   Surrogate splits:
##       gre &lt; 530   to the left,  agree=0.582, adj=0.121, (0 split)
##       gpa &lt; 3.225 to the right, agree=0.553, adj=0.061, (0 split)
## 
## Node number 3: 192 observations,    complexity param=0.06299213
##   predicted class=0  expected loss=0.4270833  P(node) =0.48
##     class counts:   110    82
##    probabilities: 0.573 0.427 
##   left son=6 (160 obs) right son=7 (32 obs)
##   Primary splits:
##       rank splits as  RLLL,      improve=8.0083330, (0 missing)
##       gre  &lt; 450   to the left,  improve=1.1737770, (0 missing)
##       gpa  &lt; 3.945 to the left,  improve=0.5037879, (0 missing)
## 
## Node number 4: 99 observations
##   predicted class=0  expected loss=0.1313131  P(node) =0.2475
##     class counts:    86    13
##    probabilities: 0.869 0.131 
## 
## Node number 5: 109 observations,    complexity param=0.01574803
##   predicted class=0  expected loss=0.293578  P(node) =0.2725
##     class counts:    77    32
##    probabilities: 0.706 0.294 
##   left son=10 (99 obs) right son=11 (10 obs)
##   Primary splits:
##       gre  &lt; 730   to the left,  improve=3.63727200, (0 missing)
##       gpa  &lt; 2.905 to the left,  improve=0.33841060, (0 missing)
##       rank splits as  RL--,      improve=0.02221607, (0 missing)
## 
## Node number 6: 160 observations,    complexity param=0.02362205
##   predicted class=0  expected loss=0.3625  P(node) =0.4
##     class counts:   102    58
##    probabilities: 0.637 0.362 
##   left son=12 (89 obs) right son=13 (71 obs)
##   Primary splits:
##       rank splits as  -RLL,      improve=1.4024450, (0 missing)
##       gre  &lt; 650   to the left,  improve=1.1404990, (0 missing)
##       gpa  &lt; 3.945 to the left,  improve=0.7032468, (0 missing)
##   Surrogate splits:
##       gpa &lt; 3.515 to the right, agree=0.594, adj=0.085, (0 split)
## 
## Node number 7: 32 observations
##   predicted class=1  expected loss=0.25  P(node) =0.08
##     class counts:     8    24
##    probabilities: 0.250 0.750 
## 
## Node number 10: 99 observations
##   predicted class=0  expected loss=0.2525253  P(node) =0.2475
##     class counts:    74    25
##    probabilities: 0.747 0.253 
## 
## Node number 11: 10 observations
##   predicted class=1  expected loss=0.3  P(node) =0.025
##     class counts:     3     7
##    probabilities: 0.300 0.700 
## 
## Node number 12: 89 observations
##   predicted class=0  expected loss=0.3033708  P(node) =0.2225
##     class counts:    62    27
##    probabilities: 0.697 0.303 
## 
## Node number 13: 71 observations,    complexity param=0.02362205
##   predicted class=0  expected loss=0.4366197  P(node) =0.1775
##     class counts:    40    31
##    probabilities: 0.563 0.437 
##   left son=26 (55 obs) right son=27 (16 obs)
##   Primary splits:
##       gpa &lt; 3.495 to the right, improve=2.600032, (0 missing)
##       gre &lt; 500   to the left,  improve=1.751006, (0 missing)
## 
## Node number 26: 55 observations,    complexity param=0.01574803
##   predicted class=0  expected loss=0.3636364  P(node) =0.1375
##     class counts:    35    20
##    probabilities: 0.636 0.364 
##   left son=52 (26 obs) right son=53 (29 obs)
##   Primary splits:
##       gpa &lt; 3.73  to the left,  improve=2.8948640, (0 missing)
##       gre &lt; 690   to the right, improve=0.3878788, (0 missing)
##   Surrogate splits:
##       gre &lt; 610   to the left,  agree=0.582, adj=0.115, (0 split)
## 
## Node number 27: 16 observations
##   predicted class=1  expected loss=0.3125  P(node) =0.04
##     class counts:     5    11
##    probabilities: 0.312 0.688 
## 
## Node number 52: 26 observations
##   predicted class=0  expected loss=0.1923077  P(node) =0.065
##     class counts:    21     5
##    probabilities: 0.808 0.192 
## 
## Node number 53: 29 observations,    complexity param=0.01574803
##   predicted class=1  expected loss=0.4827586  P(node) =0.0725
##     class counts:    14    15
##    probabilities: 0.483 0.517 
##   left son=106 (9 obs) right son=107 (20 obs)
##   Primary splits:
##       gre &lt; 690   to the right, improve=0.8827586, (0 missing)
##       gpa &lt; 3.945 to the left,  improve=0.5029606, (0 missing)
## 
## Node number 106: 9 observations
##   predicted class=0  expected loss=0.3333333  P(node) =0.0225
##     class counts:     6     3
##    probabilities: 0.667 0.333 
## 
## Node number 107: 20 observations
##   predicted class=1  expected loss=0.4  P(node) =0.05
##     class counts:     8    12
##    probabilities: 0.400 0.600</code></pre>
</div>
<p>At each stage of the game the decision tree is trying to <strong>partition</strong> the data into two sets based upon some “rule. Each set is given a”prediction“. If the split can successfully improve discrimination beyond a certain threshold then it is added. There are usually restrictions on both <strong>depth</strong> and <strong>bin size</strong>. For example, the process, typically, stops trying when it reaches a depth of 30, or when the partition under consideration has 5 or fewer observations. This generates a tree that is, almost certianly, too complicated (recall what we said about <em>over fitting</em> earlier).</p>
<p>The second step is to <strong>trim</strong> the tree. The <code>rpart</code> function does some of this automatically– you might wish to do some more. The <code>prune()</code> function allows you to do this</p>
<p>Read <a href="https://dzone.com/articles/how-to-create-a-perfect-decision-tree">this DZone article on decision trees</a></p>
<p><strong>Exercise:</strong> Now read <a href="http://uc-r.github.io/regression_trees">this article</a> and reproduce their code (no copy-pasting allowed!) which discusses using regression trees to categorize more than just a binary response. It’s the same basic idea, but the prediction is for more than just 0 or 1. Notice that the <code>rpart()</code> function is using <code>method=&quot;anova&quot;</code> rather than <code>method=&quot;class&quot;</code></p>
<p>DEFINE: CROSS-VALIDATION ERROR PRESS STATISTIC MSE</p>
<pre><code>## Loading required package: tidyr</code></pre>
<pre><code>## 
## Attaching package: &#39;tidyr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:magrittr&#39;:
## 
##     extract</code></pre>
<pre><code>## The following object is masked from &#39;package:Matrix&#39;:
## 
##     expand</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:kableExtra&#39;:
## 
##     group_rows</code></pre>
<pre><code>## The following object is masked from &#39;package:MASS&#39;:
## 
##     select</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:qpcR&#39;:
## 
##     RMSE</code></pre>
<p><img src="Linear_Regression_files/figure-html/regression-exercise-external%20exercise-1.png" width="624" /><img src="Linear_Regression_files/figure-html/regression-exercise-external%20exercise-2.png" width="624" /> ## Random Forests</p>
<p>This is a pretty small data set. When the data set is larger there may be conflicting choices, more noise, and other issues that make producing the optimal decision tree problematic. One approach is to pick a random subset of the data (rows <strong>and</strong> columns) and build a tree from that subset. Repeat this process many, many times. The rows are selected <strong>with replacement</strong> (This is known as <strong>bootstrapping</strong>).</p>
<p>I want to reiterate a few details about the <strong>random subset</strong> idea. For random forests this is more than just selecting random rows with replacement. The explanatory variables are ALSO selected at random. As the many random trees are produced, the various explanatory variables involved in the rules are monitored and a running tally of <strong>the most important variables</strong> are produced.</p>
<p>There are many <strong>aggregation techniques</strong> that do something similar. Bootstrap aggregation (<strong>bagging</strong>) is one such example. However, randomly producing trees can generate many trees that are too similar. This reduces the effectiveness of the technique. The <strong>random forest</strong> idea</p>
<p><strong>EXERCISE:</strong> Read the <a href="https://uc-r.github.io/random_forests">following tutorial from uc-r</a> and reproduce their example. Do <strong>NOT</strong> copy and paste.</p>
<pre><code>## randomForest 4.6-14</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<pre><code>## 
## Attaching package: &#39;ranger&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:randomForest&#39;:
## 
##     importance</code></pre>
<pre><code>## 
## ----------------------------------------------------------------------
## 
## Your next step is to start H2O:
##     &gt; h2o.init()
## 
## For H2O package documentation, ask for help:
##     &gt; ??h2o
## 
## After starting H2O, you can use the Web UI at http://localhost:54321
## For more information visit http://docs.h2o.ai
## 
## ----------------------------------------------------------------------</code></pre>
<pre><code>## 
## Attaching package: &#39;h2o&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cor, sd, var</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     &amp;&amp;, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,
##     colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,
##     log10, log1p, log2, round, signif, trunc</code></pre>
<p>POTENTIALLY Read <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">Leo Breiman and ADele Cutler’s manual on Random Forests</a>.</p>
</div>
<div id="section-boosted-regression" class="section level2">
<h2>Boosted Regression</h2>
<p>The package <code>gbm</code> will perform boosted regression and the summary provides a nice summary of the relative influence of the explanatory variables:</p>
<pre><code>## Loaded gbm 2.1.5</code></pre>
<pre><code>## Distribution not specified, assuming bernoulli ...</code></pre>
<img src="Linear_Regression_files/figure-html/unnamed-chunk-32-1.png" width="624" />
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["var"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["rel.inf"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"gpa","2":"41.98683","_rn_":"gpa"},{"1":"rank","2":"30.61267","_rn_":"rank"},{"1":"gre","2":"27.40051","_rn_":"gre"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="section-regression-trees" class="section level2">
<h2>Regression Trees</h2>
<p>Rather than just attempting to classify, the partitioning technique can <em>also</em> break the explanatory-data space into subspaces… each with their own regression line… think of it as a decision tree whose leaves result in a data set and a regression on that set.</p>
<p>NOTE:<br />
DISCUSS DIFFERENCE BETWEEN CLASSIFICATION TREE AND REGRESSION TREE MAKE SOME PLOTS ADD SOME EXERCISES DRAWBACKS AND SHORTCOMINGS OF THIS APPROACH (RANDOM NATURE OF CUTS TOO)</p>
<div id="section-exercise-with-hint" class="section level3">
<h3>Exercise with Hint</h3>
<p><em>Here’s an exercise where the chunk is pre-evaulated via the <code>exercise.eval</code> option (so the user can see the default output we’d like them to customize). We also add a “hint” to the correct solution via the chunk immediate below labeled <code>print-limit-hint</code>.</em></p>
<p>Modify the following code to limit the number of rows printed to 5:</p>
<div class="tutorial-exercise" data-label="print-limit" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>mtcars</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["mpg"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["cyl"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["disp"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["hp"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["drat"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["wt"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["qsec"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["vs"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["am"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["gear"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["carb"],"name":[11],"type":["dbl"],"align":["right"]}],"data":[{"1":"21.0","2":"6","3":"160.0","4":"110","5":"3.90","6":"2.620","7":"16.46","8":"0","9":"1","10":"4","11":"4","_rn_":"Mazda RX4"},{"1":"21.0","2":"6","3":"160.0","4":"110","5":"3.90","6":"2.875","7":"17.02","8":"0","9":"1","10":"4","11":"4","_rn_":"Mazda RX4 Wag"},{"1":"22.8","2":"4","3":"108.0","4":"93","5":"3.85","6":"2.320","7":"18.61","8":"1","9":"1","10":"4","11":"1","_rn_":"Datsun 710"},{"1":"21.4","2":"6","3":"258.0","4":"110","5":"3.08","6":"3.215","7":"19.44","8":"1","9":"0","10":"3","11":"1","_rn_":"Hornet 4 Drive"},{"1":"18.7","2":"8","3":"360.0","4":"175","5":"3.15","6":"3.440","7":"17.02","8":"0","9":"0","10":"3","11":"2","_rn_":"Hornet Sportabout"},{"1":"18.1","2":"6","3":"225.0","4":"105","5":"2.76","6":"3.460","7":"20.22","8":"1","9":"0","10":"3","11":"1","_rn_":"Valiant"},{"1":"14.3","2":"8","3":"360.0","4":"245","5":"3.21","6":"3.570","7":"15.84","8":"0","9":"0","10":"3","11":"4","_rn_":"Duster 360"},{"1":"24.4","2":"4","3":"146.7","4":"62","5":"3.69","6":"3.190","7":"20.00","8":"1","9":"0","10":"4","11":"2","_rn_":"Merc 240D"},{"1":"22.8","2":"4","3":"140.8","4":"95","5":"3.92","6":"3.150","7":"22.90","8":"1","9":"0","10":"4","11":"2","_rn_":"Merc 230"},{"1":"19.2","2":"6","3":"167.6","4":"123","5":"3.92","6":"3.440","7":"18.30","8":"1","9":"0","10":"4","11":"4","_rn_":"Merc 280"},{"1":"17.8","2":"6","3":"167.6","4":"123","5":"3.92","6":"3.440","7":"18.90","8":"1","9":"0","10":"4","11":"4","_rn_":"Merc 280C"},{"1":"16.4","2":"8","3":"275.8","4":"180","5":"3.07","6":"4.070","7":"17.40","8":"0","9":"0","10":"3","11":"3","_rn_":"Merc 450SE"},{"1":"17.3","2":"8","3":"275.8","4":"180","5":"3.07","6":"3.730","7":"17.60","8":"0","9":"0","10":"3","11":"3","_rn_":"Merc 450SL"},{"1":"15.2","2":"8","3":"275.8","4":"180","5":"3.07","6":"3.780","7":"18.00","8":"0","9":"0","10":"3","11":"3","_rn_":"Merc 450SLC"},{"1":"10.4","2":"8","3":"472.0","4":"205","5":"2.93","6":"5.250","7":"17.98","8":"0","9":"0","10":"3","11":"4","_rn_":"Cadillac Fleetwood"},{"1":"10.4","2":"8","3":"460.0","4":"215","5":"3.00","6":"5.424","7":"17.82","8":"0","9":"0","10":"3","11":"4","_rn_":"Lincoln Continental"},{"1":"14.7","2":"8","3":"440.0","4":"230","5":"3.23","6":"5.345","7":"17.42","8":"0","9":"0","10":"3","11":"4","_rn_":"Chrysler Imperial"},{"1":"32.4","2":"4","3":"78.7","4":"66","5":"4.08","6":"2.200","7":"19.47","8":"1","9":"1","10":"4","11":"1","_rn_":"Fiat 128"},{"1":"30.4","2":"4","3":"75.7","4":"52","5":"4.93","6":"1.615","7":"18.52","8":"1","9":"1","10":"4","11":"2","_rn_":"Honda Civic"},{"1":"33.9","2":"4","3":"71.1","4":"65","5":"4.22","6":"1.835","7":"19.90","8":"1","9":"1","10":"4","11":"1","_rn_":"Toyota Corolla"},{"1":"21.5","2":"4","3":"120.1","4":"97","5":"3.70","6":"2.465","7":"20.01","8":"1","9":"0","10":"3","11":"1","_rn_":"Toyota Corona"},{"1":"15.5","2":"8","3":"318.0","4":"150","5":"2.76","6":"3.520","7":"16.87","8":"0","9":"0","10":"3","11":"2","_rn_":"Dodge Challenger"},{"1":"15.2","2":"8","3":"304.0","4":"150","5":"3.15","6":"3.435","7":"17.30","8":"0","9":"0","10":"3","11":"2","_rn_":"AMC Javelin"},{"1":"13.3","2":"8","3":"350.0","4":"245","5":"3.73","6":"3.840","7":"15.41","8":"0","9":"0","10":"3","11":"4","_rn_":"Camaro Z28"},{"1":"19.2","2":"8","3":"400.0","4":"175","5":"3.08","6":"3.845","7":"17.05","8":"0","9":"0","10":"3","11":"2","_rn_":"Pontiac Firebird"},{"1":"27.3","2":"4","3":"79.0","4":"66","5":"4.08","6":"1.935","7":"18.90","8":"1","9":"1","10":"4","11":"1","_rn_":"Fiat X1-9"},{"1":"26.0","2":"4","3":"120.3","4":"91","5":"4.43","6":"2.140","7":"16.70","8":"0","9":"1","10":"5","11":"2","_rn_":"Porsche 914-2"},{"1":"30.4","2":"4","3":"95.1","4":"113","5":"3.77","6":"1.513","7":"16.90","8":"1","9":"1","10":"5","11":"2","_rn_":"Lotus Europa"},{"1":"15.8","2":"8","3":"351.0","4":"264","5":"4.22","6":"3.170","7":"14.50","8":"0","9":"1","10":"5","11":"4","_rn_":"Ford Pantera L"},{"1":"19.7","2":"6","3":"145.0","4":"175","5":"3.62","6":"2.770","7":"15.50","8":"0","9":"1","10":"5","11":"6","_rn_":"Ferrari Dino"},{"1":"15.0","2":"8","3":"301.0","4":"335","5":"3.54","6":"3.570","7":"14.60","8":"0","9":"1","10":"5","11":"8","_rn_":"Maserati Bora"},{"1":"21.4","2":"4","3":"121.0","4":"109","5":"4.11","6":"2.780","7":"18.60","8":"1","9":"1","10":"4","11":"2","_rn_":"Volvo 142E"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"default","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="print-limit-hint" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>head(mtcars)</code></pre>
</div>
</div>
<div id="section-quiz" class="section level3">
<h3>Quiz</h3>
<p><em>You can include any number of single or multiple choice questions as a quiz. Use the <code>question</code> function to define a question and the <code>quiz</code> function for grouping multiple questions together.</em></p>
<p>Some questions to verify that you understand the purposes of various base and recommended R packages:</p>
<p><div class="panel panel-default">
<div class="panel-heading tutorial-panel-heading">Quiz</div>
<table class="table quiz-table">
<tr>
<td>

<div id="htmlwidget-3ac8006a45d292b46f34" style="width:100%;height:auto;", class = "quiz html-widget">
<div class="panel panel-default">
<div class="panel-body quizArea">
</div>
</div>
</div>

<script type="application/json" data-for="htmlwidget-3ac8006a45d292b46f34">{"x":{"question":"Which package contains functions for installing other R packages?","answers":[{"option":"base","correct":false,"message":null},{"option":"tools","correct":false,"message":null},{"option":"utils","correct":true,"message":null},{"option":"codetools","correct":false,"message":null}],"label":"quiz-1","skipStartButton":true,"perQuestionResponseAnswers":true,"perQuestionResponseMessaging":true,"preventUnanswered":true,"displayQuestionCount":false,"displayQuestionNumber":false,"disableRanking":true,"nextQuestionText":"","checkAnswerText":"Submit Answer","allowRetry":false,"randomSortAnswers":false,"json":{"info":{"name":"","main":""},"questions":[{"q":"Which package contains functions for installing other R packages?","a":[{"option":"base","correct":false,"message":null},{"option":"tools","correct":false,"message":null},{"option":"utils","correct":true,"message":null},{"option":"codetools","correct":false,"message":null}],"correct":"Correct!","incorrect":"Incorrect."}]}},"evals":[],"jsHooks":[]}</script>
</td>
</tr>
<tr>
<td>

<div id="htmlwidget-dfbef8075aecd82fa7e5" style="width:100%;height:auto;", class = "quiz html-widget">
<div class="panel panel-default">
<div class="panel-body quizArea">
</div>
</div>
</div>

<script type="application/json" data-for="htmlwidget-dfbef8075aecd82fa7e5">{"x":{"question":"Which of the R packages listed below are used to create plots?","answers":[{"option":"lattice","correct":true,"message":null},{"option":"tools","correct":false,"message":null},{"option":"stats","correct":false,"message":null},{"option":"grid","correct":true,"message":null}],"label":"quiz-2","skipStartButton":true,"perQuestionResponseAnswers":true,"perQuestionResponseMessaging":true,"preventUnanswered":true,"displayQuestionCount":false,"displayQuestionNumber":false,"disableRanking":true,"nextQuestionText":"","checkAnswerText":"Submit Answer","allowRetry":false,"randomSortAnswers":false,"json":{"info":{"name":"","main":""},"questions":[{"q":"Which of the R packages listed below are used to create plots?","a":[{"option":"lattice","correct":true,"message":null},{"option":"tools","correct":false,"message":null},{"option":"stats","correct":false,"message":null},{"option":"grid","correct":true,"message":null}],"correct":"Correct!","incorrect":"Incorrect."}]}},"evals":[],"jsHooks":[]}</script>
</td>
</tr>
</table>
</div> ## Non parameteric regression</p>
<p>(Borrowing heavily from Chapter 18 of John Fox’s Applied Regression Analysis and Generalized Liner Models)</p>
<p>The techniques we have discussed earlier are all based on an underlying model whose exact from is determined by numeric <strong>parameters</strong>. The process of regression determines reasonable values for these parameters. There are other techniques that do not</p>
One technique that does not rely upon an underlying parametric model for the data is known as <strong>local linear regression</strong> (or <strong>loess</strong>). As with most non-parametric forms of regressin 
<script type="application/shiny-prerendered" data-context="server-start">
library(learnr)
knitr::opts_chunk$set(echo = FALSE)
# knitr::knit_hooks$set(output = function(x, options){
#   if(!is.null(options$max_height)){
#     paste('<pre style = "max-height:',
#           options$max_height, 
#           ';float: left; width: 910px; overflow-y: auto;">',
#           x,
#           "<\u002fpre>",
#           sep = "")
#   }else{
#     x
#   }
#   
# })
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::register_http_handlers(session, metadata = NULL)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
output$lmPlot <- renderPlot({
  f=function(x){
    y.value<-input$intercept+input$slope*x #possibly a vector
    #if(length(y.value)==1){cat(y.value,"\n")}
    return(y.value)
  }
  pt.loc=function(x,y,f){ #returns 1 if (x,y) is above the line y=mx+b, -1 if below and 0 if upon
    results<-sign(y-f(x))
   # cat(paste(x,y,":",results,"\n"))              
    results
  }
     x=airquality$Temp
     y=airquality$Ozone
     p.y=f(x)
     y.l=0
     y.u=170
     x.l=50
     x.u=105
     # We need to ensure that lower two points on bounding box are below line 
     if(pt.loc(x.l,y.l,f)>0 | pt.loc(x.u,y.l,f)>0){
       y.l=min(f(x.l),f(x.u))
     }
     #Also must ensure that upper two points on bounding box are above line
     if(pt.loc(x.l,y.u,f)<1 | pt.loc(x.u,y.u,f)<1){
       y.u=max(f(x.l),f(x.u))
     }
     plot(y~x,
         main="New York air quality May to September of 1973",
         xlab="Temperature (deg F)",ylab="Ozone (ppb)",
         xlim=c(x.l,x.u),
         ylim=c(y.l,y.u)
    )
    abline(input$intercept,input$slope,lwd=2)
    if(input$showResiduals){
           segments(x,y,x,p.y,col="lightgray")
    }
    if(input$showSS){
      ss=sum((p.y-y)^2,na.rm=TRUE)
      ss=round(ss,1)
      text(60,150,paste("ss:",ss))
    }
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-two-plus-two-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-two-plus-two-code-editor`)), session)
output$`tutorial-exercise-two-plus-two-output` <- renderUI({
  `tutorial-exercise-two-plus-two-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-print-limit-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-print-limit-code-editor`)), session)
output$`tutorial-exercise-print-limit-output` <- renderUI({
  `tutorial-exercise-print-limit-result`()
})
</script>
 <!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/cerulean.min.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css","rstudio-theme.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["font-awesome"]},{"type":"character","attributes":{},"value":["5.1.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/fontawesome"]}]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["css/all.css","css/v4-shims.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["4.4.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["localforage"]},{"type":"character","attributes":{},"value":["1.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/localforage"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["localforage.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ionrangeslider"]},{"type":"character","attributes":{},"value":["2.1.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["href"]}},"value":[{"type":"character","attributes":{},"value":["shared/ionrangeslider"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/ion.rangeSlider.min.js"]},{"type":"character","attributes":{},"value":["css/ion.rangeSlider.css","css/ion.rangeSlider.skinShiny.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"logical","attributes":{},"value":[true]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["strftime"]},{"type":"character","attributes":{},"value":["0.9.2"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["href"]}},"value":[{"type":"character","attributes":{},"value":["shared/strftime"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["strftime-min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"logical","attributes":{},"value":[true]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ionrangeslider"]},{"type":"character","attributes":{},"value":["2.1.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["href"]}},"value":[{"type":"character","attributes":{},"value":["shared/ionrangeslider"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/ion.rangeSlider.min.js"]},{"type":"character","attributes":{},"value":["css/ion.rangeSlider.css","css/ion.rangeSlider.skinShiny.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"logical","attributes":{},"value":[true]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["strftime"]},{"type":"character","attributes":{},"value":["0.9.2"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["href"]}},"value":[{"type":"character","attributes":{},"value":["shared/strftime"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["strftime-min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"logical","attributes":{},"value":[true]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["htmlwidgets"]},{"type":"character","attributes":{},"value":["1.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["www"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["htmlwidgets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["htmlwidgets"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["plotly-binding"]},{"type":"character","attributes":{},"value":["4.9.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["htmlwidgets"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["plotly.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["plotly"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["4.9.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["typedarray"]},{"type":"character","attributes":{},"value":["0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["htmlwidgets/lib/typedarray"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["typedarray.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["plotly"]},{"type":"logical","attributes":{},"value":[false]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["crosstalk"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.0.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["crosstalk"]},{"type":"character","attributes":{},"value":["1.0.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["www"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/crosstalk.min.js"]},{"type":"character","attributes":{},"value":["css/crosstalk.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["crosstalk"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.0.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["plotly-htmlwidgets-css"]},{"type":"character","attributes":{},"value":["1.46.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["htmlwidgets/lib/plotlyjs"]}]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["plotly-htmlwidgets.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["plotly"]},{"type":"logical","attributes":{},"value":[false]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["plotly-main"]},{"type":"character","attributes":{},"value":["1.46.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["htmlwidgets/lib/plotlyjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["plotly-latest.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["plotly"]},{"type":"logical","attributes":{},"value":[false]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["htmlwidgets"]},{"type":"character","attributes":{},"value":["1.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["www"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["htmlwidgets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["htmlwidgets"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["quiz-binding"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["htmlwidgets"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["quiz.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/bootstrap.min.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["4.4.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["localforage"]},{"type":"character","attributes":{},"value":["1.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/localforage"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["localforage.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["slickquiz"]},{"type":"character","attributes":{},"value":["1.5.20"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["htmlwidgets/lib/slickquiz"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/slickQuiz.js"]},{"type":"character","attributes":{},"value":["css/slickQuiz.css","css/slickQuizTutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["kePrint"]},{"type":"character","attributes":{},"value":["0.0.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["kePrint-0.0.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["kePrint.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["kableExtra"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.1.0"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["htmlwidgets"]},{"type":"character","attributes":{},"value":["1.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["www"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["htmlwidgets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["htmlwidgets"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.3"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["quiz-binding"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["htmlwidgets"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["quiz.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/bootstrap.min.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["1.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["4.4.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["localforage"]},{"type":"character","attributes":{},"value":["1.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/localforage"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["localforage.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.9.2.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["slickquiz"]},{"type":"character","attributes":{},"value":["1.5.20"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["htmlwidgets/lib/slickquiz"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/slickQuiz.js"]},{"type":"character","attributes":{},"value":["css/slickQuiz.css","css/slickQuizTutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.9.2.1"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112]}},"value":[{"type":"character","attributes":{},"value":["AmesHousing","assertthat","backports","base","bitops","caret","class","codetools","colorspace","compiler","crayon","crosstalk","data.table","datasets","DEoptimR","digest","dplyr","evaluate","foreach","furrr","future","gbm","generics","ggplot2","globals","glue","gower","graphics","grDevices","grid","gridExtra","gtable","h2o","highr","hms","htmltools","htmlwidgets","httpuv","httr","ipred","iterators","jsonlite","kableExtra","knitr","later","lattice","lava","lazyeval","learnr","listenv","lubridate","magrittr","manipulateWidget","markdown","MASS","Matrix","methods","mime","miniUI","minpack.lm","ModelMetrics","munsell","nlme","nnet","parallel","pillar","pkgconfig","plotly","plyr","prodlim","promises","purrr","qpcR","R6","randomForest","ranger","Rcpp","RCurl","readr","recipes","reshape2","rgl","rlang","rmarkdown","robustbase","rpart","rpart.plot","rprojroot","rsample","rstudioapi","rvest","scales","shiny","splines","stats","stats4","stringi","stringr","survival","tibble","tidyr","tidyselect","timeDate","tools","utils","viridisLite","webshot","withr","xfun","xml2","xtable","yaml"]},{"type":"character","attributes":{},"value":["0.0.3","0.2.1","1.1.4","3.5.3","1.0-6","6.0-84","7.3-15","0.2-16","1.4-1","3.5.3","1.3.4","1.0.0","1.12.2","3.5.3","1.0-8","0.6.19","0.8.1","0.14","1.4.4","0.1.0","1.14.0","2.1.5","0.0.2","3.2.0","0.12.4","1.3.1","0.2.1","3.5.3","3.5.3","3.5.3","2.3","0.3.0","3.24.0.5","0.8","0.4.2","0.3.6","1.3","1.5.1","1.4.0","0.9-9","1.0.10","1.6","1.1.0","1.23","0.8.0","0.20-38","1.6.5","0.2.2","0.9.2.1","0.7.0","1.7.4","1.5","0.10.0","1.0","7.3-51.4","1.2-17","3.5.3","0.7","0.1.1.1","1.2-1","1.2.2","0.5.0","3.1-140","7.3-12","3.5.3","1.4.1","2.0.2","4.9.0","1.8.4","2018.04.18","1.0.1","0.3.2","1.4-1","2.4.0","4.6-14","0.11.2","1.0.1","1.95-4.12","1.3.1","0.1.6","1.4.3","0.100.26","0.4.0","1.13","0.93-5","4.1-15","3.0.7","1.3-2","0.0.5","0.10","0.3.4","1.0.0","1.3.2","3.5.3","3.5.3","3.5.3","1.4.3","1.4.0","2.44-1.1","2.1.3","0.8.3","0.2.5","3043.102","3.5.3","3.5.3","0.3.0","0.5.1","2.1.2","0.7","1.2.0","1.8-4","2.2.0"]}]}]}
</script>
<!--/html_preserve-->
</div>
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">Regression Tutorial</h2>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>

</html>
